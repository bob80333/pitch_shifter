just realized val dl not in test mode

new runs for that
bigger / deeper model better after moving to test mode
larger batch size also better

now let's try 2 tweaks:

1. swap to l1 loss on waveform  (as the MDX model does for vocal isolation, and it does very well)
2. remove the impossible frequencies.  e.g. if shifted up then down, top freqs are missing.  other direction we just lose fidelity, not all freqs.
removing them for shifting up is important because then the model tries to predict missing freqs, as that is a lot of the loss and easy to do, but doesn't actually improve quality

(output17)
First, doing 2., as it is likely more important for quality.


(output18)
trying 1., result after 10k steps is much worse sounding audio, lots of background noise

let's try CDPAM loss, a perceptual loss

CDPAM loss takes too much memory, 1/2 the sequence length to 32256 from 65024 samples

still too much memory, 1/2 batch size from 32 to 16

(output19)
really bad results, but checked and cdpam expects sample rate of 22050
Since the lower frequencies are where the problem is, let's try resampling it 

(output20)
terrible
somehow sounds even worse
super loud as well

new idea:
rather than estimate the final output
estimate the difference
this way the model doesn't have to copy the input over AND do the other stuff
maybe it will be better?

worse, definitely worse.

output22:
back to original method, reduce sequence length to 3/4 original, so 48640 instead of 65024, gives a little speedup and still processes a decent bit of audio
up the # of lowpass filters to 10 from 6, change to using a for loop (should have done that from the beginning)
back to batch size 32

not really any better.

Let's move from spectrogram models to trying out waveform models
output23:
much faster
bottlenecked completely by data loading
lower memory usage
easier to deal with padding for inference due to no spec -> inv spec
similar setup to spectrogram model but made convs 1d

model config:
channels = [1, 8, 64, 256, 512]
blocks = [2, 2, 3, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]

not too much worse than spectrogram model on spectrogram val loss
(0.515 vs 0.491)
audio sounds pretty similar, no real improvement in sound

output24:
new change: ConvNext blocks have convs of size 7.  7x7 works for images, but 7x7 = 49, so conv of 41 is a similar prime # conv size for 1d
should allow model to see higher freqs and move information along the sequence faster
due to convnext structure this doesn't add many params (only +~200k on a ~21.5M param model)
similarly fast training (bottlenecked by data loading)

slightly worse results

output25:
since model is so fast and results are still worse than spec model,
let's add more blocks
blocks = [4, 4, 4, 4]
now all depths have same # of blocks
keeping the larger kernel for now, not clear why it would be worse esp for waveform audio model
e.g. hifigan does good with a kernel of 11 with 11 dilation giving huge effective kernel

training is slower but still somewhat dataloading bound
still low memory use, <5GB

we see increase in val loss after only 5k steps, and little to no progress after 1k steps
maybe the issue is too high of lr?

reduce lr: muon from 2e-3 to 1e-3, adamw from 3e-4 to 1e-4

results noticably worse

increase lr?
output27
muon to 5e-3, adamw to 5e-4
much better results
at 5k steps achieves val loss of 0.508, which is better than any other wav model attempt after 10k steps (next best @ 10k steps, 0.5153, output23)

new best result, beating even spectrogram model in this setting.

output28
let's try raising lr 2x:
muon to 1e-2, adamw to 1e-3

worse.

more depth:
output29
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 32, 128, 256, 512]
blocks = [4, 4, 4, 4, 4]
factors = [8, 4, 4, 2, 2]
scale_vs_channels = [1, 1, 1, 1, 1]

model a little bigger, a little more memory, <6GB

less depth:

output30:
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 64, 128, 256]
blocks = [4, 4, 4, 4]
factors = [8, 8, 2, 2]
scale_vs_channels = [1, 1, 1, 1]

much smaller (from ~22M params to ~6M params)
faster
worse

looking at bigvgan paper, a few things to try
1. gradient clipping:

grad clip norm of 1e3, same as bigvgan paper uses, 
output31

maybe track grad norms to see whats typical?

very similar results
makes sense, model wasn't unstable. so it didn't really have much of an effect
model val loss was monotonically decreasing though, so that's nice

looking at DAC (neural audio codec) paper, they do various losses, a spectrogram loss, feature matching, etc
the GAN approach is tempting, it has been proven to give great results in audio
but, the real idea is from melgan, multi level feature matching loss on discriminator.
but training discriminator is expensive and takes 1-2 MILLION steps to converge

instead, let's use some of the encoding layers from a neural codec like DAC or EnCodec.
Pros: Due to compression / reconstruction objective, tries to model the most perceptually relevant parts of the signal already
is a pretrained model
can handle high sample rate (trained on 44.5khz, 48khz )

output32
new: dac_loss.py implementing DACFeatureMatchingLoss
unfortunately using all layers + the 49152 samples destroyed memory
reducing to after just the first EncoderBlock (unfortunately first encoder block is without any downsampling so memory usage quite bad) and 16384 samples makes it fit in <12GB memory
given model was using <5-6GB before and went to using >24GB, something like 75% of memory use here is from the DAC loss :/

promising, had a different sound to it than spec loss
but plataeued and val spec loss increased then plataeued, didn't decrease
output33
trying multiple encoder blocks, may be able to fit in memory
can only fit 2 blocks
output34
new idea, feature match loss after every residual block
just use first block

quality wise they sound softer, less of the harsh robotic tone that the pitch shifting causes, but with weird artifacts
let's try this with the multi scale stft loss, so both losses at once
using just first block with feat matching loss after every residual
the scale of the stft loss is much larger (100x) than the scale of the feature matching loss, 
initially trying with no scaling of either loss to see if the strong stft loss can prevent the artifacts from the feature matching loss
output35

prevents the artifacts, but doesn't really sound better

let's try the multiscale mel spec loss from the DAC paper
they note that you need different # mel bins to prevent "holes" in the spectrogram
and that the shorter window lengths are important for transients
so this may help

output36

maybe? idk hard to tell with short audios

let's try same length as before the DACFeatureMatchingLoss, 16384*3 samples

output37

muon is supposed to have worse later convergence, let's try out this PSGD optimizer

gonna have to try a few things to tune hparams
noticably slower, probably due to not compiling (windows moment)

lots of loss spikes after like 1k steps with lr of 1e-3
output38

trying 3e-4 lr
bit of a slow start, and stalls out later
output39
trying 5e-4 lr
worse overall, loss spikes later
output40
maybe lower lr?
1e-4
ends up worse than other lrs, but monotonically decreasing val loss at least
output41

back to 3e-4, try out different momentum
default was 0.9, let's try 0.8
initially promising results, but falls behind in the 2nd half and val loss goes up and down later on
output42

default of 0.9, let's try 0.95
ends up very similar to the others tbh
output43

new dataset setup, preprocess data instead of on the fly shifting
to reduce disk usage, reduce # of shifts
# only deal with large shifts for now
shift = [-12, -11, -10, 10, 11, 12]
the largest shifts are the ones we care about most anyways
since they are the ones that have the most artifacts
preprocessing does 2x the number of lowpass filters since might as well (more filters = faster / better rolloff / smaller transition from allowed to blocked) (and they are not the slow part anyways)

have to redo comparisons

Muon:
Muon(muon_params, lr=5e-3, momentum=0.95,
                 adamw_params=adamw_params, adamw_lr=5e-4, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

also due to faster dataloading will increase # of train steps to 20k
(went from ~2.6it/s just loading data with 2 workers to almost 40it/s just loading data)
finally reduced model first level to only 3 blocks intead of 4 to try to speed it up a bit

output44

PSGD:

ForeachPSGDKron(model.parameters(), lr=3e-4, beta = 0.95, weight_decay=0.01)

output45

Let's try out the SOAP optimizer.  as a 2nd order optimizer its supposed to be quite good
use adam settings:
lr 5e-4, betas = 0.9, 0.95
output46

Let's try out plain old AdamW, to compare against all these other optimizers
lr 5e-4, betas = 0.9, 0.95
output47

AdamW and SOAP underperformed vs Muon & PSGD significantly

PSGD underperformed a little

AdamW lr may be incorrect
lets try increasing it (SOAP is slower so use AdamW to tune first)

AdamW lr 1e-3
much better, results got much closer to psgd / muon
output48

AdamW lr 2e-3
similar results but faster beginning
output49

AdamW lr 5e-3
immediate and constant loss explosions
deleted run since no useful information

will use 1e-3 as adam lr

let's try to tune muon a bit more now with new adam lr and set muon to 10x adam lr
so muon from 5e-3 to 1e-2

optimizer = Muon(muon_params, lr=1e-2, momentum=0.95,
                adamw_params=adamw_params, adamw_lr=1e-3, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

much better, beats all previous runs by 6k steps / 20k
output50

let's try SOAP with new lr 1e-3

does OK
massive loss spike ~12k-13k steps in
stalls out for a while
gets a nice improvement in val loss in the last 2k steps, but too late
although does manage to beat SOAP with lower lr at the very end
output51

let's try PSGD with higher lr, all optimizers are doing better with higher lr with new dataset setup

lr = 1e-3

loss explosion pretty quickly

lr = 5e-4?
basically same result as 3e-4 lr
output52

OK, let's try much longer training runs.
AdamW, 100k steps
output53

Muon
output54

PSGD
output55

just realized that the audio isn't getting properly low-passed for pitch shifted baseline


fix audio preprocessing
output56

fix preprocessing actually (Muon)
output57

try longer run to see if better result (Muon)
not really, a little better
output58

add seeds / reproducibility, reduce LR a bit
output59

try out wavlm feature matching loss
the paper "FINALLY: fast and universal speech enhancement with studio-like quality" uses wavlm conv features
they show that wavlm features allow them to train a decent quality autoencoder, notably CDPAM completely fails to train an autoencoder (which matches my experience)
they come up with metrics to measure how good features are and show that wavlm performs well on those metrics (CDPAM fails them)

they say that wavlm features + stft loss allows them to train a model with an MOS score approaching that of adversarial training (4.3 vs 4.6) without any discriminator
problem: wavlm features only work for 16khz.  so they use wavlm features for stage 1 of 3, and then not for later stages which use discriminators and 48khz audio

for this test I will just resample audio to 16khz in dataloader and reduce # of samples processed to 1/3 (same factor as sample rate reduction)
if it works, I will (in another repo) attempt to distill wavlm conv features into another model

48khz audio -> student -> features
16khz audio -> wavlm_conv -> features
loss on features
in theory gives us student that can handle 48khz audio and has good properties like wavlm
but first need to see if worth it to even try

does manage to train model
results not noticably better than multi stft loss
metrics worse, not surprising, very different loss function for training vs val
output60

let's try wavlm features + the melspec loss i was using before
the wavlm loss gets down to ~7e-6 by the end of 10k steps
the melspec loss gets down to ~0.97 = ~9.7e-1 by the end of 10k steps
to keep the loss scales similar, multiply wavlm loss by 1e5
output61

train for longer
didn't do better
output62

try fixing skip connections
1. conv that takes channels * 2 and outputs channels (kernel = 1)
also added sisdr for validation, it is inverted since it is a loss, so lower is better
since nothing is directly optimizing anything like sisdr, it should be more useful to tell quality
output63

2. conv on the residual, then add
like conv(res) + x
output64

either way performance not great.  maybe this architecture is bad
let's try melgan based architecture
(good audio archs are often very different from good vision archs)
rough start but caught up by the end
output65

some tweaks to fix the architecture, it wasn't quite correct
worse somehow
also both melgan based models had high pitched artifacts
output66

many sound separation models predict a mask they then multiply
try that at end of model? start with melgan
model NaN'd out ~300 steps in, gradient norms died to 0 after ~65 steps
output67

let's try again with original unet model?
trained successfully, but worse results
output68

all these tweaks and the model is still failing to fix the audio
to make the task easier, let's make training data only have 1 shift, same as test (the 1 octave shift)
remove wavlm loss, try with just stft, to make training / iterating faster

output69

overall, the artifacts are not going away.
arch, loss, optimizer, 10k steps or 100k steps, still same artifact as original audio

let's try GAN training, there is a reason every audio paper uses one

notes_gan.txt


ok, let's try model_1d.py without multiplication
muon, 5k steps, batch 32
output70

modified architecture following more closely (but not the same) setup as hifi++ wavunet
same otherwise
wow, really good spectrogram matching, the melspec looks identical
but still sounds bad
output71

l1 loss? modified arch
not really, l1 loss just isn't very great perceptually
output72

if not the power spectra then its the phase!
phase loss from dac library
killed after 2k steps... my ears!!!!
output73

modified arch,  melspec + l1 loss
lambda_l1 = 10, since its ~0.025 and melspec is ~1 after training, brings them within 1 oom
looks like the l1 might be helping
output74

20k steps
better, sounds slightly better than baseline
but nowhere near original
output75

modified the v2 model to be faster and have more params (less blocks at high res, more channels at low res, more downsampling)
5k steps
just mel spec loss
similar metrics
output 76