just realized val dl not in test mode

new runs for that
bigger / deeper model better after moving to test mode
larger batch size also better

now let's try 2 tweaks:

1. swap to l1 loss on waveform  (as the MDX model does for vocal isolation, and it does very well)
2. remove the impossible frequencies.  e.g. if shifted up then down, top freqs are missing.  other direction we just lose fidelity, not all freqs.
removing them for shifting up is important because then the model tries to predict missing freqs, as that is a lot of the loss and easy to do, but doesn't actually improve quality

(output17)
First, doing 2., as it is likely more important for quality.


(output18)
trying 1., result after 10k steps is much worse sounding audio, lots of background noise

let's try CDPAM loss, a perceptual loss

CDPAM loss takes too much memory, 1/2 the sequence length to 32256 from 65024 samples

still too much memory, 1/2 batch size from 32 to 16

(output19)
really bad results, but checked and cdpam expects sample rate of 22050
Since the lower frequencies are where the problem is, let's try resampling it 

(output20)
terrible
somehow sounds even worse
super loud as well

new idea:
rather than estimate the final output
estimate the difference
this way the model doesn't have to copy the input over AND do the other stuff
maybe it will be better?

worse, definitely worse.

output22:
back to original method, reduce sequence length to 3/4 original, so 48640 instead of 65024, gives a little speedup and still processes a decent bit of audio
up the # of lowpass filters to 10 from 6, change to using a for loop (should have done that from the beginning)
back to batch size 32

not really any better.

Let's move from spectrogram models to trying out waveform models
output23:
much faster
bottlenecked completely by data loading
lower memory usage
easier to deal with padding for inference due to no spec -> inv spec
similar setup to spectrogram model but made convs 1d

model config:
channels = [1, 8, 64, 256, 512]
blocks = [2, 2, 3, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]

not too much worse than spectrogram model on spectrogram val loss
(0.515 vs 0.491)
audio sounds pretty similar, no real improvement in sound

output24:
new change: ConvNext blocks have convs of size 7.  7x7 works for images, but 7x7 = 49, so conv of 41 is a similar prime # conv size for 1d
should allow model to see higher freqs and move information along the sequence faster
due to convnext structure this doesn't add many params (only +~200k on a ~21.5M param model)
similarly fast training (bottlenecked by data loading)

slightly worse results

output25:
since model is so fast and results are still worse than spec model,
let's add more blocks
blocks = [4, 4, 4, 4]
now all depths have same # of blocks
keeping the larger kernel for now, not clear why it would be worse esp for waveform audio model
e.g. hifigan does good with a kernel of 11 with 11 dilation giving huge effective kernel

training is slower but still somewhat dataloading bound
still low memory use, <5GB

we see increase in val loss after only 5k steps, and little to no progress after 1k steps
maybe the issue is too high of lr?

reduce lr: muon from 2e-3 to 1e-3, adamw from 3e-4 to 1e-4

results noticably worse

increase lr?
output27
muon to 5e-3, adamw to 5e-4
much better results
at 5k steps achieves val loss of 0.508, which is better than any other wav model attempt after 10k steps (next best @ 10k steps, 0.5153, output23)

new best result, beating even spectrogram model in this setting.

output28
let's try raising lr 2x:
muon to 1e-2, adamw to 1e-3

worse.

more depth:
output29
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 32, 128, 256, 512]
blocks = [4, 4, 4, 4, 4]
factors = [8, 4, 4, 2, 2]
scale_vs_channels = [1, 1, 1, 1, 1]

model a little bigger, a little more memory, <6GB

less depth:

output30:
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 64, 128, 256]
blocks = [4, 4, 4, 4]
factors = [8, 8, 2, 2]
scale_vs_channels = [1, 1, 1, 1]

much smaller (from ~22M params to ~6M params)
faster
worse

looking at bigvgan paper, a few things to try
1. gradient clipping:

grad clip norm of 1e3, same as bigvgan paper uses, 
output31

maybe track grad norms to see whats typical?

very similar results
makes sense, model wasn't unstable. so it didn't really have much of an effect
model val loss was monotonically decreasing though, so that's nice

looking at DAC (neural audio codec) paper, they do various losses, a spectrogram loss, feature matching, etc
the GAN approach is tempting, it has been proven to give great results in audio
but, the real idea is from melgan, multi level feature matching loss on discriminator.
but training discriminator is expensive and takes 1-2 MILLION steps to converge

instead, let's use some of the encoding layers from a neural codec like DAC or EnCodec.
Pros: Due to compression / reconstruction objective, tries to model the most perceptually relevant parts of the signal already
is a pretrained model
can handle high sample rate (trained on 44.5khz, 48khz )

output32
new: dac_loss.py implementing DACFeatureMatchingLoss
unfortunately using all layers + the 49152 samples destroyed memory
reducing to after just the first EncoderBlock (unfortunately first encoder block is without any downsampling so memory usage quite bad) and 16384 samples makes it fit in <12GB memory
given model was using <5-6GB before and went to using >24GB, something like 75% of memory use here is from the DAC loss :/

promising, had a different sound to it than spec loss
but plataeued and val spec loss increased then plataeued, didn't decrease
output33
trying multiple encoder blocks, may be able to fit in memory
can only fit 2 blocks
output34
new idea, feature match loss after every residual block
just use first block

quality wise they sound softer, less of the harsh robotic tone that the pitch shifting causes, but with weird artifacts
let's try this with the multi scale stft loss, so both losses at once
using just first block with feat matching loss after every residual
the scale of the stft loss is much larger (100x) than the scale of the feature matching loss, 
initially trying with no scaling of either loss to see if the strong stft loss can prevent the artifacts from the feature matching loss
output35

prevents the artifacts, but doesn't really sound better

let's try the multiscale mel spec loss from the DAC paper
they note that you need different # mel bins to prevent "holes" in the spectrogram
and that the shorter window lengths are important for transients
so this may help

output36

maybe? idk hard to tell with short audios

let's try same length as before the DACFeatureMatchingLoss, 16384*3 samples

output37

muon is supposed to have worse later convergence, let's try out this PSGD optimizer

gonna have to try a few things to tune hparams
noticably slower, probably due to not compiling (windows moment)

lots of loss spikes after like 1k steps with lr of 1e-3
output38

trying 3e-4 lr
bit of a slow start, and stalls out later
output39
trying 5e-4 lr
worse overall, loss spikes later
output40
maybe lower lr?
1e-4
ends up worse than other lrs, but monotonically decreasing val loss at least
output41

back to 3e-4, try out different momentum
default was 0.9, let's try 0.8
initially promising results, but falls behind in the 2nd half and val loss goes up and down later on
output42

default of 0.9, let's try 0.95
ends up very similar to the others tbh
output43

new dataset setup, preprocess data instead of on the fly shifting
to reduce disk usage, reduce # of shifts
# only deal with large shifts for now
shift = [-12, -11, -10, 10, 11, 12]
the largest shifts are the ones we care about most anyways
since they are the ones that have the most artifacts
preprocessing does 2x the number of lowpass filters since might as well (more filters = faster / better rolloff / smaller transition from allowed to blocked) (and they are not the slow part anyways)

have to redo comparisons

Muon:
Muon(muon_params, lr=5e-3, momentum=0.95,
                 adamw_params=adamw_params, adamw_lr=5e-4, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

also due to faster dataloading will increase # of train steps to 20k
(went from ~2.6it/s just loading data with 2 workers to almost 40it/s just loading data)
finally reduced model first level to only 3 blocks intead of 4 to try to speed it up a bit

output44

PSGD:

ForeachPSGDKron(model.parameters(), lr=3e-4, beta = 0.95, weight_decay=0.01)

output45

Let's try out the SOAP optimizer.  as a 2nd order optimizer its supposed to be quite good
use adam settings:
lr 5e-4, betas = 0.9, 0.95
output46

Let's try out plain old AdamW, to compare against all these other optimizers
lr 5e-4, betas = 0.9, 0.95
output47

AdamW and SOAP underperformed vs Muon & PSGD significantly

PSGD underperformed a little

AdamW lr may be incorrect
lets try increasing it (SOAP is slower so use AdamW to tune first)

AdamW lr 1e-3
much better, results got much closer to psgd / muon
output48

AdamW lr 2e-3
similar results but faster beginning
output49

AdamW lr 5e-3
immediate and constant loss explosions
deleted run since no useful information

will use 1e-3 as adam lr

let's try to tune muon a bit more now with new adam lr and set muon to 10x adam lr
so muon from 5e-3 to 1e-2

optimizer = Muon(muon_params, lr=1e-2, momentum=0.95,
                adamw_params=adamw_params, adamw_lr=1e-3, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

much better, beats all previous runs by 6k steps / 20k
output50

let's try SOAP with new lr 1e-3

does OK
massive loss spike ~12k-13k steps in
stalls out for a while
gets a nice improvement in val loss in the last 2k steps, but too late
although does manage to beat SOAP with lower lr at the very end
output51

let's try PSGD with higher lr, all optimizers are doing better with higher lr with new dataset setup

lr = 1e-3

loss explosion pretty quickly

lr = 5e-4?
basically same result as 3e-4 lr
output52

OK, let's try much longer training runs.
AdamW, 100k steps
output53

Muon
output54

PSGD
output55

just realized that the audio isn't getting properly low-passed for pitch shifted baseline


fix audio preprocessing
output56

fix preprocessing actually (Muon)
output57

try longer run to see if better result (Muon)
not really, a little better
output58

add seeds / reproducibility, reduce LR a bit
output59

try out wavlm feature matching loss
the paper "FINALLY: fast and universal speech enhancement with studio-like quality" uses wavlm conv features
they show that wavlm features allow them to train a decent quality autoencoder, notably CDPAM completely fails to train an autoencoder (which matches my experience)
they come up with metrics to measure how good features are and show that wavlm performs well on those metrics (CDPAM fails them)

they say that wavlm features + stft loss allows them to train a model with an MOS score approaching that of adversarial training (4.3 vs 4.6) without any discriminator
problem: wavlm features only work for 16khz.  so they use wavlm features for stage 1 of 3, and then not for later stages which use discriminators and 48khz audio

for this test I will just resample audio to 16khz in dataloader and reduce # of samples processed to 1/3 (same factor as sample rate reduction)
if it works, I will (in another repo) attempt to distill wavlm conv features into another model

48khz audio -> student -> features
16khz audio -> wavlm_conv -> features
loss on features
in theory gives us student that can handle 48khz audio and has good properties like wavlm
but first need to see if worth it to even try

does manage to train model
results not noticably better than multi stft loss
metrics worse, not surprising, very different loss function for training vs val
output60

let's try wavlm features + the melspec loss i was using before
the wavlm loss gets down to ~7e-6 by the end of 10k steps
the melspec loss gets down to ~0.97 = ~9.7e-1 by the end of 10k steps
to keep the loss scales similar, multiply wavlm loss by 1e5
output61

train for longer
didn't do better
output62

try fixing skip connections
1. conv that takes channels * 2 and outputs channels (kernel = 1)
also added sisdr for validation, it is inverted since it is a loss, so lower is better
since nothing is directly optimizing anything like sisdr, it should be more useful to tell quality
output63

2. conv on the residual, then add
like conv(res) + x
output64

either way performance not great.  maybe this architecture is bad
let's try melgan based architecture
(good audio archs are often very different from good vision archs)
rough start but caught up by the end
output65

some tweaks to fix the architecture, it wasn't quite correct
worse somehow
also both melgan based models had high pitched artifacts
output66

many sound separation models predict a mask they then multiply
try that at end of model? start with melgan
model NaN'd out ~300 steps in, gradient norms died to 0 after ~65 steps
output67

let's try again with original unet model?
trained successfully, but worse results
output68

all these tweaks and the model is still failing to fix the audio
to make the task easier, let's make training data only have 1 shift, same as test (the 1 octave shift)
remove wavlm loss, try with just stft, to make training / iterating faster

output69

overall, the artifacts are not going away.
arch, loss, optimizer, 10k steps or 100k steps, still same artifact as original audio

let's try GAN training, there is a reason every audio paper uses one

notes_gan.txt


ok, let's try model_1d.py without multiplication
muon, 5k steps, batch 32
output70

modified architecture following more closely (but not the same) setup as hifi++ wavunet
same otherwise
wow, really good spectrogram matching, the melspec looks identical
but still sounds bad
output71

l1 loss? modified arch
not really, l1 loss just isn't very great perceptually
output72

if not the power spectra then its the phase!
phase loss from dac library
killed after 2k steps... my ears!!!!
output73

modified arch,  melspec + l1 loss
lambda_l1 = 10, since its ~0.025 and melspec is ~1 after training, brings them within 1 oom
looks like the l1 might be helping
output74

20k steps
better, sounds slightly better than baseline
but nowhere near original
output75

modified the v2 model to be faster and have more params (less blocks at high res, more channels at low res, more downsampling)
5k steps
just mel spec loss
similar metrics
output 76

back to the melspec + l1 setting but with 100k steps and bigger batch size (64)
since its the only result that had real promise for making the audio sound actually good
and it was still having loss / metric improvements at 20k steps

really great results actually
unfortunately after testing on a full audio there was artifacts
maybe due to too short training length

output77

so:
increase train length 4x to 65536 (like bigvgan v2 config)
reduce batch size 4x
increase the validation length to 16*16384 (~5.4 seconds, close to the length that caused issues)
output78

realization: if you upsample the audio to 96khz before shifting, then you can keep all the high frequencies intact for any shift <= +1 octave (+12 semitones)
so redo data preprocessing to fix this
this way model can be trained to always output fullband audio.
during testing, when testing on a sample, the scores for the model were actually worse than the shifted audio
as due to the low pass filter on training data, the model actually removed some of the high frequencies still present
10k steps: does it work
output79: yes

100k steps
output80

let's try increasing batch size a bit, from 16 to 32, since muon benefits from larger batch size
maybe reduce data size from 4*16384 to 3*16384 to help speed up training a bit (still ~1 second of audio so it sees all frequencies)
also reduce val length from 16x to 12x 16k, the >5 seconds length ends up creating a lot of padding for shorter audios, and makes eval kinda slow
pretty good results
output81

installed triton for windows, see how much faster training goes
train for 5k steps, muon should be compiled.  based on previous test the model inferences a batch at ~19it/s
so ideally training speed is 1/3 that, but loss/optim takes some time
training at ~4.2-4.3 it/s
now trying compiling the model
when testing on inference in the model testing code, speeds up model by ~2x to ~35it/s!
only speeds up training to ~5it/s
either optimizer has large overhead
or mel loss has large overhead
without mel loss gets to ~5.5it/s
without mel loss and without muon (AdamW, but not fused adam) gtes to ~8.1-8.2it/s
adamw with mel loss gets ~7.1-7.2it/s (fused has similar speed)
so muon has large overhead here
let's try adamw for 100k steps to see if with enough steps it approaches muon results
output82

since the unshift_down work has found a better architecture setup, inspired by the DAC setup, let's see if it works better for removing artifacts as well.
and switch back to muon for it
# params: 34.92M
channels = [48, 96, 192, 384, 768]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 4
output83
short run to test it out, 10k steps instead of usual 100k
a little worse than output81

maybe try alternative option:
less patching, but fewer channels to maintain speed
channels = [16, 32, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 4, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 2
output84
trains faster than other option
decent results

muon has had an update, adds weight decay, try it out
had to fix by commenting out distributed stuff
also terrible results
reverted to commit 92bc50fb8749f12cd3869ce60c984c3bbe844cf5
and it works again
trying commit 28c793b55ef1cf86e5d6091bfbdbe0029b11eabb (next commit)
works fine
trying commit eb225792d1da44848dff39f9fac9758a71ee3f18 (next commit)
doesn't work (due to distributed stuff)
trying commit fc20399079ef3570ecf7ba2f12cbb82cc3464e86 (next muon.py commit)
breaks, but had to comment out some code about all gather
maybe need to actually do distributed stuff
can't on windows
modified the code to work without all gather, see how it does
output85
not as good :/
big spike in grad norm, loss is worse and has a small bump

try 2 of modification 
output86
still not working
initially gets ok si-sdr but then just fails to learn


swap to local editable install
latest commit + my modification for non distributed
output87

learns, but is noticably worse
loss is pretty similar but a bit worse
val metrics are worse and they aren't improving
handful of large spikes in grad norm

checkout commit 92bc50fb8749f12cd3869ce60c984c3bbe844cf5
try that
output 88 
works

ok, got advice from eleutherai discord
wrapped convs in class that reshapes the weights
since new muon handles 3d weights differently now
now trying to train with that setup
output89
works! and results are slightly better!

For average improvement benchmark
with output84:
 E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - -4.64016886295264 Subtraction - -13.569113724843126
Step:  2000
Average improvements: Model - -4.604981892745814 Subtraction - -13.569113724843126
Step:  3000
Average improvements: Model - -9.264409568461618 Subtraction - -13.569113724843126
Step:  4000
Average improvements: Model - -4.92021777790467 Subtraction - -13.569113724843126
Step:  5000
Average improvements: Model - -5.6688230394693395 Subtraction - -13.569113724843126
Step:  6000
Step:  7000
Average improvements: Model - -8.447322318212168 Subtraction - -13.569113724843126
Step:  8000
Average improvements: Model - -11.264506773855418 Subtraction - -13.569113724843126
Step:  9000
Average improvements: Model - -9.866973060463645 Subtraction - -13.569113724843126
Step:  10000
Average improvements: Model - -10.613895883690292 Subtraction - -13.569113724843126


With output89:
E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - 0.31719582521259504 Subtraction - -13.569113724843126
Step:  2000
Average improvements: Model - -1.5929669580740597 Subtraction - -13.569113724843126
Step:  3000
Average improvements: Model - -4.639959124448916 Subtraction - -13.569113724843126
Step:  4000
Average improvements: Model - -5.794628661815162 Subtraction - -13.569113724843126
Step:  5000
Average improvements: Model - -10.138142818013568 Subtraction - -13.569113724843126
Step:  6000
Average improvements: Model - -7.555109908064983 Subtraction - -13.569113724843126
Step:  7000
Average improvements: Model - -6.107469998312544 Subtraction - -13.569113724843126
Step:  8000
Average improvements: Model - -8.13147475796058 Subtraction - -13.569113724843126
Step:  9000
Average improvements: Model - -7.005445790777264 Subtraction - -13.569113724843126
Step:  10000
Average improvements: Model - -6.731054338322272 Subtraction - -13.569113724843126


Better at most steps, at step 1000 has a slightly positive score!


Now, output90
same as output89, with updated muon
and using the new feature, weight decay in muon!
I had set it to 0 for testing, but default is 0.01, and the LLM muon 2x speed paper found it critical
so setting it to 0.01, same as adamw

similar results, which is good / expected
should only really come into play in longer runs to reduce overfitting

PS E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - -3.3375640283382975 Subtraction - -13.56
Step:  2000
Average improvements: Model - -9.085865686564793 Subtraction - -13.569
Step:  3000
Average improvements: Model - -6.631856726540372 Subtraction - -13.569
Step:  4000
Average improvements: Model - -9.119680613752772 Subtraction - -13.569
Step:  5000
Average improvements: Model - -8.239379045059371 Subtraction - -13.569
Step:  6000
Step:  8000
Average improvements: Model - -2.6199704498009506 Subtraction - -13.563724843126
Step:  9000
Average improvements: Model - -7.6327673905560065 Subtraction - -13.563724843126
Step:  10000
Average improvements: Model - -6.836417576744688 Subtraction - -13.569724843126

weight decay maybe hurts results a bit early on, should help with longer training

let's try 100k steps
output91




can we make a faster good model? faster training -> can do longer training runs with larger batch sizes
(batch 32, 3*16384 samples, 200 forward passes timed (randomized input each forward))
current model_1d_v2 setup:  Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 11.4979989528656 seconds

model_1d_dac Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 16.86350131034851 seconds

model_1d_spec Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 3.2860000133514404 seconds
this model is super fast but doesn't touch phase

model_1d_spec_phase Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 6.290999412536621 seconds
Does same thing to phase as spec with different set of params
unsurprisingly, takes twice as long, but since original is super fast, still pretty quick


head to head model comparisons
at end of training, do si-sdr on whole dataset, and production quality metric from audiobox aesthetics.
training for 20k steps at batch 32 with size of 3*16384 during training (~1 second 48khz), and 12*16384 during testing (~4.1 seconds 48khz)

also, the validation mel loss now matches training loss, already have si-sdr for alternative metric anyways
additionally added spectrogram visualization like in other setup, helps see if any obvious artifacts from model, and see difference in model artifacts between architectures

output92 is the model_1d_v2 model
output93 is the model_1d_dac model
first training of model_1d_dac exploded after ~2.2k steps
lowering lr from 5e-3 muon 5e-4 adamw to 1e-3 muon 1e-4 adamw (original DAC model uses 1e-4 as LR)
output94 is the model_1d_dac model, with lower lr (hopefully no loss / grad norm explosion)
output95 is the model_1d_spec model
output96 is the model_1d_spec_phase model

we will compare models by looking at the PQ and si-sdr values

What, mean, std, median, min, max

model_1d_v2:

Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.100630060727917, 0.3200090286296923, 7.1264989376068115, 5.572937965393066, 8.147261619567871
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -11.26012295092323, 9.334433749448385, -9.199989795684814, -73.98777770996094, 5.0

model_1d_dac

Original PQ, 7.397888841802504, 0.27755410045302487, 7.426485061645508, 5.6817240715026855, 8.255695343017578
Shifted PQ, 6.341449345559327, 0.6139276536274133, 6.361863374710083, 4.283082962036133, 8.050267219543457
Unshifted PQ, 7.156263238569694, 0.3083742135394702, 7.18623685836792, 5.835231304168701, 8.145949363708496
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.289102757941732, 9.227971339065173, -8.289327144622803, -67.4070053100586, 5.25086784362793

skipping the spec models since they are pretty bad, barely if at all improve over shifted in PQ and si-sdr

used a t-test website between model_1d_v2 and model_1d_dac
just using summary stats, not original values get this:

P value and statistical significance:
The two-tailed P value is less than 0.0001
By conventional criteria, this difference is considered to be extremely statistically significant.
Confidence interval:
The mean of 1D_V2 minus 1D_DAC equals -0.055633177841777000
95% confidence interval of this difference: From -0.068967065916809540 to -0.042299289766744460
Intermediate values used in calculations:
t = 8.1974
df = 8574
standard error of difference = 0.007 

so model_1d_dac is better in PQ
could have used paired t-test since we are working on same audio samples between both models, and all the values are saved

model_1d_v2 also has an issue in spectrogram, where there is a line across middle pretty strongly, probably related to the patching or upsampling / downsampling layers aliasing

Anyways, so now the problem is that the DAC-based model is kinda slow

DAC uses a different upsampling / downsampling than other model does, maybe can use that to reduce length initially?

Original DAC speed / memory usage:
19177MiB
~2.6it/s

DAC with 1 2x downsampling / upsampling inserted at beginning (stride = 2, initial_channels = 16, no change in model strides)
11320MiB
~4.0 it/s

let's train this DAC model and see how good it is, the speedup / memory reduction is quite nice, hopefully doesn't hurt performance too much
output97

also to note, DAC codebase configs use audio segments of 0.38seconds ~= 16384 samples, this setup is currently training on 3x that
could potentially reduce and get some speedup / memory reduction, could maybe increase batch size as well?
not clear if batchsize*#samples is what matters, or if more batchsize less samples is better, or if less batchsize more samples is better
EVA-GAN seems to think #samples more important, we can try those changes out later

DAC with the downsampling and nothing else
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.130573544929277, 0.315028272943776, 7.159674406051636, 5.63200569152832, 8.066680908203125
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.780254906943586, 9.169527232640597, -8.700427055358887, -79.99737548828125, 7.1225175857543945


P value and statistical significance:
The two-tailed P value equals 0.0001
By conventional criteria, this difference is considered to be extremely statistically significant.
Confidence interval:
The mean of DAC original minus DAC downsample equals 0.025689693640417000
95% confidence interval of this difference: From 0.012463007487406676 to 0.038916379793427325
Intermediate values used in calculations:
t = 3.8160
df = 8574
standard error of difference = 0.007 

about half the distance between the DAC model and the 1d_v2 model
so with the downsampling the DAC model is a little faster to train than the 1d_v2 model and still gets better results

Now, let's compare it when I reduce training seqlen, keeping batch size the same and not
so 2 more runs:

DAC 2x downsample, seqlen 1/3 = 16384, batch = 32 - output98
uses 4654MiB memory
~7.5it/s
note: GPU utilization seems to be hovering ~50-60%, so this is underutilizing GPU, which is why its not even 2x faster training
on the other hand, this potentially could allow to increase model size without slowdown, if it turns out this is a good training setup?
Also, maybe need to do some profiling to see what's the bottleneck in allowing full GPU utilization?
DAC 2x downsample, seqlen 1/3 = 16384, batch = 96 (3x) - output99

overall results: original setup is much better quality than other 2

what if: reduce batch size by 1/2? batch size seems to affect quality less than # samples

output100