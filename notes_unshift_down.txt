initial testing:
muon
tf32
10k steps
batch 32, 16384*2 samples
model_1d_v2

outputs_unshift_down/output0
sounds pretty bad
metrics line up with that
wasn't fully converged, better results with more training

took 29 minutes to train 10k steps
but maybe spectrogram based model would do better here?

model_2d_v2
-256 samples from testing / training to make spectrogram shape right

took like 1.5 minutes to compile 2d model
ran into a crash with 2d model for validation
RuntimeError: setStorage: sizes [32, 512, 512, 2], strides [524288, 1024, 2, 1], storage offset 0, and itemsize 8 requiring a storage size of 134217728 are out of bounds for storage of size 67108864   
turning off torch compile, may be the cause
ok, runs without torch compile
outputs_unshift_down/output1

overall, metrics worse than 1d model
and slower because can't use torch compile

train 1d model for 20k steps instead of 10k steps, see how it improves
outputs_unshift_down/output2

only a little better

maybe need wider / deeper model?

more depth:
add more blocks to each level / add more levels
1) add more blocks to each level
previous:
blocks = [1, 3, 4, 4]
bottleneck = 4
# params: 15.83M
now:
blocks = [2, 4, 6, 6]
bottleneck = 6
# params: 22.79M
10k steps
outputs_unshift_down/output3

results are almost the same
the sisdr are almost exactly the same

took 43 minutes to train 10k steps


2) add more levels
making the factors smaller, making it more similar to bigvgan downsampling
# params: 62.80M
channels = [8, 32, 64, 128, 256, 512, 1024]
blocks = [1, 3, 4, 4, 4, 4]
factors = [8, 4, 2, 2, 2, 2]
scale_vs_channels = [2, 2, 1, 1, 1, 1]

bottleneck_blocks = 4

outputs_unshift_down/output4

slightly better evals
does sound better, but still pretty bad
took ~58 minutes to reach 10k steps, so took about 2x the time of baseline to get slightly better results

new idea: use the architecture setup from DAC.  Since it is an actual Unet without skip connections (autoencoder), it should be better for this task
DAC uses 64 channels at base layer, making it very slow.  let's try 32. can always widen later
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3

DAC architecture uses the parallel blocks from HiFiGAN, so 3 blocks deep, but 3 parallel branches.
just using normal block setup for now, to see if the channel / factor setup works better

# params: 16.26M
so similar params to original setup. makes sense, as most params are at deeper layers, and this setup has same width at the end
quite slow unfortunately

unfortunately when trying to train with this setup, used too much memory and spilled into regular RAM

easiest way to reduce memory usage is to reduce # of blocks in first level
channels = [32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3
# params: 16.22M

now uses only ~21GB of vram to train with seqlen 32k and batch size 32
outputs_unshift_down/output5

wow the results are actually better! by 4k steps its basically matching best stft val loss, and by 5k steps its beaten them
by 6k steps has best sisdr validation yet

took 1 hr 19 minutes, slowest yet

unfortunately the 21GB vram means can't increase batch size or seq len for larger scale training run 
would have to probably reduce # channels. at least in the first levels
maybe something like:
channels = [16, 32, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 4, 4]
bottleneck_blocks = 3
# params: 15.96M
let's try that to see if we can get similar results without as much VRAM usage / faster training
outputs_unshift_down/output6

similar results
a little faster, 57 minutes
used <15GB vram, so nice reduction

let's try reducing channels even more:

channels = [8, 16, 64, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 2, 4]
should be even faster / less vram
bottleneck_blocks = 3

# params: 14.70M

outputs_unshift_down/output7
used <9GB, so can probably double batch size and maybe also increase seqlen without OOMing
also much faster, probably ~60% faster it/s
slightly worse metrics
took 37 minutes, so noticably faster

let's try another setup, where we do alternating (4, 2) reductions
like this:
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [4, 2, 4, 2, 4, 2]
scale_vs_channels = [2, 1, 2, 1, 2, 1]
bottleneck_blocks = 3

# params: 12.56M
even faster then previous setup, a bit less params
hopefully does well, it probably uses less vram and trains quickly
outputs_unshift_down/output8
~6GB vram use, trains maybe 10% faster than previous setup
the vram reduction means batch size increase and seqlen increase can definitely happen

results are worse, more in line with previous.  it seems lower reductions initially are better for this task.
33 minutes for 10k steps

instead of 4, 2 try 2,4?
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [2, 4, 2, 4, 2, 4]
scale_vs_channels = [1, 2, 1, 2, 1, 2]
bottleneck_blocks = 3
outputs_unshift_down/output9

# params: 13.40M
<9 GB
results still not great

overall pitch shifting seems like quite the difficult task for this model to learn


new idea:
speed comes from reducing the # of samples quickly, quality comes from having blocks at original sample res

what if we do a 4x pixelshuffle1d down initially, before even the input / output convs
so then we have 4 channels, of 1/4 resolution.
basically like patching
should give us nice speedup, and we can increase # of channels and blocks at highest res due to the downsampling making it more efficient
and then we do this after

channels = [16, 32, 64, 128, 512]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 2]
# params: 9.78M

outputs_unshift_down/output10
<7GB vram
pretty fast model, hopefully performs well
performs better than output7, around on par with output9 but faster and lighter
maybe instead of 4x down just 2x down?
35 minutes to train

should still be pretty fast but maybe better results
outputs_unshift_down/output11
good results, still has speedup
56 minutes to train

maybe not enough channels for the downsampling?
try 4x down but more channels:
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]
4x down
runs/outputs_unshift_down/output12
# params: 15.67M
40 minutes to train

decent, but not quite at the level of output11

now new thoughts:
for most audio model only norm is weightnorm
no layernorm / instancenorm / batchnorm
basically all of them hurt results
this model uses layernorm due to convnext doing so
let's try removing it and see what happens
could maybe add weightnorm if it breaks everything
also maybe try replacing the GELU with snake/snakebeta activations which is supposed to be good for audio

keep output12 setup for now since it trains decently quick and also has decent results

runs/outputs_unshift_down/output13 no layernorm
similar results, maybe a little worse? no loss explosion or anything
slower for a while, but basically catches up at the end
maybe slightly spikier (both lower and higher) grad norm

try with weightnorm?
since literally everything uses it

runs/outputs_unshift_down/output14 no layernorm, added weightnorm
basically same results, but slower
spikier validations
43 minutes to train

so keep no layernorm, its faster and same results

next add snake activation, the DAC paper shows a nice improvement switching from LRELU to it
like from 6.92 si-sdr to 9.12 si-sdr, so like 2.2 si-sdr improvement just from snake activation

only activation in model is GELU, so replace it with snake
runs/outputs_unshift_down/output15
# params: 15.68M
overall, doing worse than with GELU.

ablate kernel size in convnext block:
1) kernel = 7 (like original convnext / same as dac)
2) kernel = 23 (bigger, but between current choice and original)
3) kernel = 41 (current choice)
4) kernel = 97 (biggest prime under 97, also close to 2x to see if larger helps)

finally:
scale up kernels with block
so:
block1 = 7
block2 = 23
block3 = 41

first, kernel=7
runs/outputs_unshift_down/output16
a few minutes faster
maybe a little worse results
worse si-sdr, same stft

second kernel = 23
runs/outputs_unshift_down/output17

kernel=41 is baseline, output13

kernel = 97
runs/outputs_unshift_down/output18

now blocks = 7, block2 = 23, block3 = 41
runs/outputs_unshift_down/output19
good results, better than all the rest and a bit faster due to smaller kernels on first 2 blocks

what if 4 blocks, and 4th block was the 97 kernel?
runs/outputs_unshift_down/output20
# params: 19.15M
worse results and decently slower

take the output19 setup but increase width again:
channels = [48, 96, 192, 384, 768]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 4
kernels = [7, 23, 41]
# params: 34.92M
runs/outputs_unshift_down/output21
gets best results but slow

new idea: hybrid model like hifi++

spectrogram model and then wav model
except unlike previous 2d model, the phase will be skipped by spectrogram model and only handled by 1d model
this is because phase is very different from magnitude, so it's better not handled by spec model
hopefully is improvement.
the idea is that the pitch shifting can mostly happen in the spectrogram part, where pitch is mostly represented
and then the wav part cleans up artifacts / fixes the phase
Hybrid model uses reduced width for each model, as there are two separate models being run
results in fewer params.
# params: 8.61M

wav_channels = [8, 16, 64, 128, 256]
wav_blocks = [3, 3, 3, 3]
wav_factors = [2, 4, 4, 8]
wav_scale_vs_channels = [1, 1, 2, 4]
wav_bottleneck = 3
wav_patching = 2

spec_channels = [4, 32, 64, 128, 256]
spec_blocks = [1, 3, 4, 4]
spec_factors = [4, 2, 2, 2]
spec_scale_vs_channels = [2, 2, 2, 2]

runs/outputs_unshift_down/output22

initial headstart, but loses it to early plateau.
maybe due to lack of parameters vs other models?


increase wav channels, see if that helps:
wav_channels = [16, 32, 128, 256, 512]
# params: 20.09M
runs/outputs_unshift_down/output23
notably, the stft loss is improved, but not really the si-sdr
with bigger 1d model, gets best stft loss, better si-sdr but still worse than other models, and is slower
is the 2d model hurting the phase somehow?
doing some graphing of phase with scripts/example_plot_phase.py

copying phase over from shifted, with original magnitude, and inverted spec gives si-sdr loss of ~2.3 much better than the ~20 currently getting
(si-sdr loss so lower is better)

still poor results, let's tune 2d model

comment out wav model to speed up training
2d model only result:
runs/outputs_unshift_down/output24

noticed that padding has error, is (1, padding) instead of (0, padding) in call
try (0, padding), see what the difference is
runs/outputs_unshift_down/output25
killed a little early, basically same results

tried messing around with copying phase more, results:
copying phase from shifted while giving good si-sdr sounds terrible
probably the perfect amplitude reconstruction is why
random phase sounds fine, but noisy
0 phase sounds ok, slightly weird but fine otherwise
brings the si-sdr from ~-2.3 to ~+3.3 and sounds much much better, not perfect, voice sounds a bit weird but way better than copying phase
so!
let's try out 2d model, but replace phase with 0s. (future hybrid model may also do this, it may be easier to reconstruct phase from amplitude than fix it)
runs/outputs_unshift_down/output26, replacing phase with 0s

helps with training loss a lot, but doesn't really help in metrics much
probably model isn't good enough at fixing spectrogram
let's try re-configuring it
setup used to be:

spec_channels = [4, 32, 64, 128, 256]
spec_blocks = [1, 3, 4, 4]
spec_factors = [4, 2, 2, 2]
spec_scale_vs_channels = [2, 2, 2, 2]

now:

spec_channels = [8, 16, 32, 64, 512]
spec_blocks = [1, 3, 3, 3]
spec_factors = [2, 2, 2, 4]
spec_scale_vs_channels = [2, 2, 2, 2]
# params: 9.67M
bottleneck_blocks = 3

runs/outputs_unshift_down/output27
let's see how this does

not great

perhaps a problem is that the STFT has limited frequency resolution
48khz / 512 bins = each bin is ~93.7hz in size
for high frequencies this is fine
but f0 of speech is probably around ~200hz
an error of even 1/2 bin = ~47hz is hugely off!
changing stft parameters of stft part to fix this.
Also, realized that hop size is wrong, to fix will add mel scale + inverse mel scale.

One at a time, let's add mel scale / fix the stft numbers

new params for stft:

# spectrogram conversion
self.to_spec = T.Spectrogram(1024, 1024, 256, power=None)
self.to_wav = T.InverseSpectrogram(1024, 1024, 256)

# mel conversion
self.to_mel = T.MelScale(n_mels = 64, sample_rate=48_000, n_stft=513)
self.to_hz = T.InverseMelScale(n_stft=513, n_mels=64, sample_rate=48_000)

Smaller input height, so should use less memory, maybe faster too
must reduce mel # to ensure no zeroes, otherwise get NaNs when inverting
initially wanted 256 mel bins, but 64 was biggest amount that didn't nan

STFT hop size is now 1/4 window (correct for hann window), so no aliasing
mel scale should emphasize frequencies that are more important to perception

runs/outputs_unshift_down/output28 

next run, change STFT / mel params to increase # bins

much bigger stft, now 2049 bins, bin size = ~23hz
# spectrogram conversion
self.to_spec = T.Spectrogram(4096, 4096, 1024, power=None)
self.to_wav = T.InverseSpectrogram(4096, 4096, 1024)

doubled the # of mels, but since mel scale is less precise at larger frequencies don't need to go all the way up
# mel conversion
self.to_mel = T.MelScale(n_mels = 512, sample_rate=48_000, n_stft=2049)
self.to_hz = T.InverseMelScale(n_stft=2049, n_mels=512, sample_rate=48_000)


actually never mind all that
try out audio transformer
just natten with 1d kernel of 127
relative postion embeddings
patching=64
(if that's too much can try less or wider model)
runs/outputs_unshift_down/output29

maybe too big patching
patch size 32:
runs/outputs_unshift_down/output30

still bad

OK!
NEW IDEA!
the model needs to be able to copy any input bin to any output bin to shift the spectrogam amplitudes.
(inspired by how the signalsmith method works)
this part can be done independently of any bins to the left or right, and then can share info across bins separately
in other words, put spectrogram into model like so:
batch, channels, n_bins, n_frames -> batch*channels, n_bins, n_frames
n_bins -> model_dim (> n_bins)
so:
conv to bring it up to model dim

then the 1d conv blocks!

then conv to bring it out

and kernel=7 is enough for sharing across bins

super fast since only a few layers
and like 20M params since model is 768 wide (nbins = 513)
runs/outputs_unshift_down/output31
lower lr
add spectrogram logging to see

hmm, weird lines in output Spectrogram
is it the 0 phase?
let's try randn phase * 0.1 to scale it down a bit
output32

train l1 loss directly on spectrogram amplitudes model deals with?
maybe will help with weird lines in spectrogram
output33

well no lines
because its all noise
pretty bad results

mel spec loss + l1 loss on model spectrogram?
better
output34

let's take it as far as we can
2x batch size (64), increase model depth and width a little
(4 blocks 768 wide to 6 blocks 1024 wide, but reduce expansion from 4x to 2x to prevent param count from exploding too much (~26M instead of ~51M))
also reduce noise in phase by another 10x, it's too much (now multiplied by 1e-3)
output35

reduce batch size back to 32
back to 0s for phase
increase width and reduce expansion once again:
width = 1536, expansion = 1
# params: 29.99M
slightly increases param count
output36
not really any better


ah, the lines are the models attempt to add the missing noise!
what if I give it a noise source?
double the input channels in input conv, make randn_like amplitude spectrogram, concat along channels
output37
worse actually

spec model 2:
instead of the convnext style block
use block style from hifi++

act -> conv -> add residual

width = 1024, 1536 didn't seem to help

start with 4 blocks, then try 8, then try 16 (see how depth scaling helps)
then try 4, 8, 16 blocks with width 1536 to see if width scaling helps

use leaky relu since spectrogram
(slope 0.1 like hifi++)

output38

still bad

just realized: this is basically 1/2 shifting, 1/2 bandwidth extension
since top half of frequencies is missing from input!

anyways:
8 blocks

output39

massive gradient norm spikes, total failure to train

new model, based off dac architecture, but uses Hifi++ setup for residual connections / upsampling
1/2 the model width compared to DAC, and decoder same # of channels as encoder, unlike dac
Also no weight norm.  
1. because Muon may be able to not need it
2. because the weird weight storage needed for muon to work breaks weightnorm :/

output40

unfortunately grad norm explodes

add weight norm, switch to adamw
output41
not great results and still had to lower lr

maybe just need to lower lr?
remove wn, back to muon:
better than adamw, and trained stably
but still not amazing results

conclusion: the bandwidth extension part doesn't work great
and normal pitch shifting has that problem anyways
so:
v2 of the unshift dataset will resample audio to 24khz to reduce spectrum, before resampling to 48khz
this way the shifted up audio will use full spectrum
also will make resample function of pytorch use much more passes, default is 6 but people in torchaudio issues said 100-200 is better
so 200, this is preprocessing

Switching to v2 dataset, re-running with output19 setup, which had some of the best results vs training time

model_1d_v2,
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 4

runs/outputs_unshift_down/output43

still bad results

so it's not the dataset making the models do bandwidth extension, its not the models (all the models have had issues, some were better than others but overall all had problems)
therefore it must be the losses
I think there's a few options
1) the mel spectrogram losses don't hit all the frequencies, so weird artifacts can show up
2) the l1 loss isn't great for hitting the stuff that the mel losses miss, since it isn't just the phase that's having problems
for the removing artifacts directly it worked well since the model didn't have to change the output much
l1 mostly just helped phase issues
and due to skip connections copying input over was easy for the model, but in this case we can't do that.

so:
firstly, I will try just a plain STFT loss with no mel bins
then try adding that to all the mel losses with a appropriate scaling
see how that goes

if that doesn't work, then I can try adding in the GAN losses, they are a staple in audio models for a reason.

runs/outputs_unshift_down/output44, only use the plain stft loss
stft_loss_plain = STFTLoss(2048, 512, 2048)

results are pretty similar, but worse
listening to the output, sounds a bit worse and has an annoying high pitched artifact

the outputs from output43 sound like the type of artifact that a GAN model would actually fix
so, I will be trying out GAN training for this

but before I do that, I will train the output 43 setup for longer to see if the artifacts will go away just with more training
looking at all previous attempts, the longest training run was only 20k steps and all the way back at output2, one of the earliest training runs
there has been progress in model architecture since then, and also the new dataset, which while not much different in metrics, sounds much worse.
this is likely due to artifacting for higher frequencies where the model also has to do the bandwidth extension tasks, since they are missing from the input after it is shifted down.

Let's train output43 setup, so not the plain stft loss, for 100k steps like the best artifact removal models were.

output45

definitely better, but not good enough, still has some of the artifacts in the speech that in my experience training melgan models, would go away with enough GAN training, but not without the discriminators