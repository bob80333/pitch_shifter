just realized val dl not in test mode

new runs for that
bigger / deeper model better after moving to test mode
larger batch size also better

now let's try 2 tweaks:

1. swap to l1 loss on waveform  (as the MDX model does for vocal isolation, and it does very well)
2. remove the impossible frequencies.  e.g. if shifted up then down, top freqs are missing.  other direction we just lose fidelity, not all freqs.
removing them for shifting up is important because then the model tries to predict missing freqs, as that is a lot of the loss and easy to do, but doesn't actually improve quality

(output17)
First, doing 2., as it is likely more important for quality.


(output18)
trying 1., result after 10k steps is much worse sounding audio, lots of background noise

let's try CDPAM loss, a perceptual loss

CDPAM loss takes too much memory, 1/2 the sequence length to 32256 from 65024 samples

still too much memory, 1/2 batch size from 32 to 16

(output19)
really bad results, but checked and cdpam expects sample rate of 22050
Since the lower frequencies are where the problem is, let's try resampling it 

(output20)
terrible
somehow sounds even worse
super loud as well

new idea:
rather than estimate the final output
estimate the difference
this way the model doesn't have to copy the input over AND do the other stuff
maybe it will be better?

worse, definitely worse.

output22:
back to original method, reduce sequence length to 3/4 original, so 48640 instead of 65024, gives a little speedup and still processes a decent bit of audio
up the # of lowpass filters to 10 from 6, change to using a for loop (should have done that from the beginning)
back to batch size 32

not really any better.

Let's move from spectrogram models to trying out waveform models
output23:
much faster
bottlenecked completely by data loading
lower memory usage
easier to deal with padding for inference due to no spec -> inv spec
similar setup to spectrogram model but made convs 1d

model config:
channels = [1, 8, 64, 256, 512]
blocks = [2, 2, 3, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]

not too much worse than spectrogram model on spectrogram val loss
(0.515 vs 0.491)
audio sounds pretty similar, no real improvement in sound

output24:
new change: ConvNext blocks have convs of size 7.  7x7 works for images, but 7x7 = 49, so conv of 41 is a similar prime # conv size for 1d
should allow model to see higher freqs and move information along the sequence faster
due to convnext structure this doesn't add many params (only +~200k on a ~21.5M param model)
similarly fast training (bottlenecked by data loading)

slightly worse results

output25:
since model is so fast and results are still worse than spec model,
let's add more blocks
blocks = [4, 4, 4, 4]
now all depths have same # of blocks
keeping the larger kernel for now, not clear why it would be worse esp for waveform audio model
e.g. hifigan does good with a kernel of 11 with 11 dilation giving huge effective kernel

training is slower but still somewhat dataloading bound
still low memory use, <5GB

we see increase in val loss after only 5k steps, and little to no progress after 1k steps
maybe the issue is too high of lr?

reduce lr: muon from 2e-3 to 1e-3, adamw from 3e-4 to 1e-4

results noticably worse

increase lr?
output27
muon to 5e-3, adamw to 5e-4
much better results
at 5k steps achieves val loss of 0.508, which is better than any other wav model attempt after 10k steps (next best @ 10k steps, 0.5153, output23)

new best result, beating even spectrogram model in this setting.

output28
let's try raising lr 2x:
muon to 1e-2, adamw to 1e-3

worse.

more depth:
output29
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 32, 128, 256, 512]
blocks = [4, 4, 4, 4, 4]
factors = [8, 4, 4, 2, 2]
scale_vs_channels = [1, 1, 1, 1, 1]

model a little bigger, a little more memory, <6GB

less depth:

output30:
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 64, 128, 256]
blocks = [4, 4, 4, 4]
factors = [8, 8, 2, 2]
scale_vs_channels = [1, 1, 1, 1]

much smaller (from ~22M params to ~6M params)
faster
worse

looking at bigvgan paper, a few things to try
1. gradient clipping:

grad clip norm of 1e3, same as bigvgan paper uses, 
output31

maybe track grad norms to see whats typical?

very similar results
makes sense, model wasn't unstable. so it didn't really have much of an effect
model val loss was monotonically decreasing though, so that's nice

looking at DAC (neural audio codec) paper, they do various losses, a spectrogram loss, feature matching, etc
the GAN approach is tempting, it has been proven to give great results in audio
but, the real idea is from melgan, multi level feature matching loss on discriminator.
but training discriminator is expensive and takes 1-2 MILLION steps to converge

instead, let's use some of the encoding layers from a neural codec like DAC or EnCodec.
Pros: Due to compression / reconstruction objective, tries to model the most perceptually relevant parts of the signal already
is a pretrained model
can handle high sample rate (trained on 44.5khz, 48khz )

output32
new: dac_loss.py implementing DACFeatureMatchingLoss
unfortunately using all layers + the 49152 samples destroyed memory
reducing to after just the first EncoderBlock (unfortunately first encoder block is without any downsampling so memory usage quite bad) and 16384 samples makes it fit in <12GB memory
given model was using <5-6GB before and went to using >24GB, something like 75% of memory use here is from the DAC loss :/

promising, had a different sound to it than spec loss
but plataeued and val spec loss increased then plataeued, didn't decrease
output33
trying multiple encoder blocks, may be able to fit in memory
can only fit 2 blocks
output34
new idea, feature match loss after every residual block
just use first block

quality wise they sound softer, less of the harsh robotic tone that the pitch shifting causes, but with weird artifacts
let's try this with the multi scale stft loss, so both losses at once
using just first block with feat matching loss after every residual
the scale of the stft loss is much larger (100x) than the scale of the feature matching loss, 
initially trying with no scaling of either loss to see if the strong stft loss can prevent the artifacts from the feature matching loss
output35

prevents the artifacts, but doesn't really sound better

let's try the multiscale mel spec loss from the DAC paper
they note that you need different # mel bins to prevent "holes" in the spectrogram
and that the shorter window lengths are important for transients
so this may help

output36

maybe? idk hard to tell with short audios

let's try same length as before the DACFeatureMatchingLoss, 16384*3 samples

output37

muon is supposed to have worse later convergence, let's try out this PSGD optimizer

gonna have to try a few things to tune hparams
noticably slower, probably due to not compiling (windows moment)

lots of loss spikes after like 1k steps with lr of 1e-3
output38

trying 3e-4 lr
bit of a slow start, and stalls out later
output39
trying 5e-4 lr
worse overall, loss spikes later
output40
maybe lower lr?
1e-4
ends up worse than other lrs, but monotonically decreasing val loss at least
output41

back to 3e-4, try out different momentum
default was 0.9, let's try 0.8
initially promising results, but falls behind in the 2nd half and val loss goes up and down later on
output42

default of 0.9, let's try 0.95
output43