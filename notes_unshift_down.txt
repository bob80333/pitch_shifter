initial testing:
muon
tf32
10k steps
batch 32, 16384*2 samples
model_1d_v2

outputs_unshift_down/output0
sounds pretty bad
metrics line up with that
wasn't fully converged, better results with more training

took 29 minutes to train 10k steps
but maybe spectrogram based model would do better here?

model_2d_v2
-256 samples from testing / training to make spectrogram shape right

took like 1.5 minutes to compile 2d model
ran into a crash with 2d model for validation
RuntimeError: setStorage: sizes [32, 512, 512, 2], strides [524288, 1024, 2, 1], storage offset 0, and itemsize 8 requiring a storage size of 134217728 are out of bounds for storage of size 67108864   
turning off torch compile, may be the cause
ok, runs without torch compile
outputs_unshift_down/output1

overall, metrics worse than 1d model
and slower because can't use torch compile

train 1d model for 20k steps instead of 10k steps, see how it improves
outputs_unshift_down/output2

only a little better

maybe need wider / deeper model?

more depth:
add more blocks to each level / add more levels
1) add more blocks to each level
previous:
blocks = [1, 3, 4, 4]
bottleneck = 4
# params: 15.83M
now:
blocks = [2, 4, 6, 6]
bottleneck = 6
# params: 22.79M
10k steps
outputs_unshift_down/output3

results are almost the same
the sisdr are almost exactly the same

took 43 minutes to train 10k steps


2) add more levels
making the factors smaller, making it more similar to bigvgan downsampling
# params: 62.80M
channels = [8, 32, 64, 128, 256, 512, 1024]
blocks = [1, 3, 4, 4, 4, 4]
factors = [8, 4, 2, 2, 2, 2]
scale_vs_channels = [2, 2, 1, 1, 1, 1]

bottleneck_blocks = 4

outputs_unshift_down/output4

slightly better evals
does sound better, but still pretty bad
took ~58 minutes to reach 10k steps, so took about 2x the time of baseline to get slightly better results

new idea: use the architecture setup from DAC.  Since it is an actual Unet without skip connections (autoencoder), it should be better for this task
DAC uses 64 channels at base layer, making it very slow.  let's try 32. can always widen later
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3

DAC architecture uses the parallel blocks from HiFiGAN, so 3 blocks deep, but 3 parallel branches.
just using normal block setup for now, to see if the channel / factor setup works better

# params: 16.26M
so similar params to original setup. makes sense, as most params are at deeper layers, and this setup has same width at the end
quite slow unfortunately

unfortunately when trying to train with this setup, used too much memory and spilled into regular RAM

easiest way to reduce memory usage is to reduce # of blocks in first level
channels = [32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3
# params: 16.22M

now uses only ~21GB of vram to train with seqlen 32k and batch size 32
outputs_unshift_down/output5

wow the results are actually better! by 4k steps its basically matching best stft val loss, and by 5k steps its beaten them
by 6k steps has best sisdr validation yet

took 1 hr 19 minutes, slowest yet

unfortunately the 21GB vram means can't increase batch size or seq len for larger scale training run 
would have to probably reduce # channels. at least in the first levels
maybe something like:
channels = [16, 32, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 4, 4]
bottleneck_blocks = 3
# params: 15.96M
let's try that to see if we can get similar results without as much VRAM usage / faster training
outputs_unshift_down/output6

similar results
a little faster, 57 minutes
used <15GB vram, so nice reduction

let's try reducing channels even more:

channels = [8, 16, 64, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 2, 4]
should be even faster / less vram
bottleneck_blocks = 3

# params: 14.70M

outputs_unshift_down/output7
used <9GB, so can probably double batch size and maybe also increase seqlen without OOMing
also much faster, probably ~60% faster it/s
slightly worse metrics
took 37 minutes, so noticably faster

let's try another setup, where we do alternating (4, 2) reductions
like this:
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [4, 2, 4, 2, 4, 2]
scale_vs_channels = [2, 1, 2, 1, 2, 1]
bottleneck_blocks = 3

# params: 12.56M
even faster then previous setup, a bit less params
hopefully does well, it probably uses less vram and trains quickly
outputs_unshift_down/output8
~6GB vram use, trains maybe 10% faster than previous setup
the vram reduction means batch size increase and seqlen increase can definitely happen

results are worse, more in line with previous.  it seems lower reductions initially are better for this task.
33 minutes for 10k steps

instead of 4, 2 try 2,4?
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [2, 4, 2, 4, 2, 4]
scale_vs_channels = [1, 2, 1, 2, 1, 2]
bottleneck_blocks = 3
outputs_unshift_down/output9

# params: 13.40M
<9 GB
results still not great

overall pitch shifting seems like quite the difficult task for this model to learn


new idea:
speed comes from reducing the # of samples quickly, quality comes from having blocks at original sample res

what if we do a 4x pixelshuffle1d down initially, before even the input / output convs
so then we have 4 channels, of 1/4 resolution.
basically like patching
should give us nice speedup, and we can increase # of channels and blocks at highest res due to the downsampling making it more efficient
and then we do this after

channels = [16, 32, 64, 128, 512]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 2]
# params: 9.78M

outputs_unshift_down/output10
<7GB vram
pretty fast model, hopefully performs well
performs better than output7, around on par with output9 but faster and lighter
maybe instead of 4x down just 2x down?
35 minutes to train

should still be pretty fast but maybe better results
outputs_unshift_down/output11
good results, still has speedup
56 minutes to train

maybe not enough channels for the downsampling?
try 4x down but more channels:
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]
4x down
runs/outputs_unshift_down/output12
# params: 15.67M
40 minutes to train

decent, but not quite at the level of output11

now new thoughts:
for most audio model only norm is weightnorm
no layernorm / instancenorm / batchnorm
basically all of them hurt results
this model uses layernorm due to convnext doing so
let's try removing it and see what happens
could maybe add weightnorm if it breaks everything
also maybe try replacing the GELU with snake/snakebeta activations which is supposed to be good for audio

keep output12 setup for now since it trains decently quick and also has decent results

runs/outputs_unshift_down/output13 no layernorm
similar results, maybe a little worse? no loss explosion or anything
slower for a while, but basically catches up at the end
maybe slightly spikier (both lower and higher) grad norm

try with weightnorm?
since literally everything uses it

runs/outputs_unshift_down/output14 no layernorm, added weightnorm
basically same results, but slower
spikier validations
43 minutes to train

so keep no layernorm, its faster and same results

next add snake activation, the DAC paper shows a nice improvement switching from LRELU to it
like from 6.92 si-sdr to 9.12 si-sdr, so like 2.2 si-sdr improvement just from snake activation

only activation in model is GELU, so replace it with snake
runs/outputs_unshift_down/output15
# params: 15.68M
overall, doing worse than with GELU.

ablate kernel size in convnext block:
1) kernel = 7 (like original convnext / same as dac)
2) kernel = 23 (bigger, but between current choice and original)
3) kernel = 41 (current choice)
4) kernel = 97 (biggest prime under 97, also close to 2x to see if larger helps)

finally:
scale up kernels with block
so:
block1 = 7
block2 = 23
block3 = 41

first, kernel=7
runs/outputs_unshift_down/output16
a few minutes faster
maybe a little worse results
worse si-sdr, same stft

second kernel = 23
runs/outputs_unshift_down/output17

kernel=41 is baseline, output13

kernel = 97
runs/outputs_unshift_down/output18

now blocks = 7, block2 = 23, block3 = 41
runs/outputs_unshift_down/output19
good results, better than all the rest and a bit faster due to smaller kernels on first 2 blocks

what if 4 blocks, and 4th block was the 97 kernel?
runs/outputs_unshift_down/output20
# params: 19.15M
worse results and decently slower

take the output19 setup but increase width again:
channels = [48, 96, 192, 384, 768]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 4
kernels = [7, 23, 41]
# params: 34.92M
runs/outputs_unshift_down/output21

