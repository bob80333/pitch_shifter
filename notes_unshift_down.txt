initial testing:
muon
tf32
10k steps
batch 32, 16384*2 samples
model_1d_v2

outputs_unshift_down/output0
sounds pretty bad
metrics line up with that
wasn't fully converged, better results with more training

took 29 minutes to train 10k steps
but maybe spectrogram based model would do better here?

model_2d_v2
-256 samples from testing / training to make spectrogram shape right

took like 1.5 minutes to compile 2d model
ran into a crash with 2d model for validation
RuntimeError: setStorage: sizes [32, 512, 512, 2], strides [524288, 1024, 2, 1], storage offset 0, and itemsize 8 requiring a storage size of 134217728 are out of bounds for storage of size 67108864   
turning off torch compile, may be the cause
ok, runs without torch compile
outputs_unshift_down/output1

overall, metrics worse than 1d model
and slower because can't use torch compile

train 1d model for 20k steps instead of 10k steps, see how it improves
outputs_unshift_down/output2

only a little better

maybe need wider / deeper model?

more depth:
add more blocks to each level / add more levels
1) add more blocks to each level
previous:
blocks = [1, 3, 4, 4]
bottleneck = 4
# params: 15.83M
now:
blocks = [2, 4, 6, 6]
bottleneck = 6
# params: 22.79M
10k steps
outputs_unshift_down/output3

results are almost the same
the sisdr are almost exactly the same

took 43 minutes to train 10k steps


2) add more levels
making the factors smaller, making it more similar to bigvgan downsampling
# params: 62.80M
channels = [8, 32, 64, 128, 256, 512, 1024]
blocks = [1, 3, 4, 4, 4, 4]
factors = [8, 4, 2, 2, 2, 2]
scale_vs_channels = [2, 2, 1, 1, 1, 1]

bottleneck_blocks = 4

outputs_unshift_down/output4

slightly better evals
does sound better, but still pretty bad
took ~58 minutes to reach 10k steps, so took about 2x the time of baseline to get slightly better results

new idea: use the architecture setup from DAC.  Since it is an actual Unet without skip connections (autoencoder), it should be better for this task
DAC uses 64 channels at base layer, making it very slow.  let's try 32. can always widen later
channels = [32, 64, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3

DAC architecture uses the parallel blocks from HiFiGAN, so 3 blocks deep, but 3 parallel branches.
just using normal block setup for now, to see if the channel / factor setup works better

# params: 16.26M
so similar params to original setup. makes sense, as most params are at deeper layers, and this setup has same width at the end
quite slow unfortunately

unfortunately when trying to train with this setup, used too much memory and spilled into regular RAM

easiest way to reduce memory usage is to reduce # of blocks in first level
channels = [32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 2, 4, 4]

bottleneck_blocks = 3
# params: 16.22M

now uses only ~21GB of vram to train with seqlen 32k and batch size 32
outputs_unshift_down/output5

wow the results are actually better! by 4k steps its basically matching best stft val loss, and by 5k steps its beaten them
by 6k steps has best sisdr validation yet

took 1 hr 19 minutes, slowest yet

unfortunately the 21GB vram means can't increase batch size or seq len for larger scale training run 
would have to probably reduce # channels. at least in the first levels
maybe something like:
channels = [16, 32, 128, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 4, 4]
bottleneck_blocks = 3
# params: 15.96M
let's try that to see if we can get similar results without as much VRAM usage / faster training
outputs_unshift_down/output6

similar results
a little faster, 57 minutes
used <15GB vram, so nice reduction

let's try reducing channels even more:

channels = [8, 16, 64, 256, 512]
blocks = [1, 3, 3, 3]
factors = [2, 4, 8, 8]
scale_vs_channels = [1, 1, 2, 4]
should be even faster / less vram
bottleneck_blocks = 3

# params: 14.70M

outputs_unshift_down/output7
used <9GB, so can probably double batch size and maybe also increase seqlen without OOMing
also much faster, probably ~60% faster it/s
slightly worse metrics
took 37 minutes, so noticably faster

let's try another setup, where we do alternating (4, 2) reductions
like this:
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [4, 2, 4, 2, 4, 2]
scale_vs_channels = [2, 1, 2, 1, 2, 1]
bottleneck_blocks = 3

# params: 12.56M
even faster then previous setup, a bit less params
hopefully does well, it probably uses less vram and trains quickly
outputs_unshift_down/output8
~6GB vram use, trains maybe 10% faster than previous setup
the vram reduction means batch size increase and seqlen increase can definitely happen

results are worse, more in line with previous.  it seems lower reductions initially are better for this task.
33 minutes for 10k steps

instead of 4, 2 try 2,4?
channels = [8, 16, 32, 64, 128, 256, 512]
blocks = [1, 3, 3, 3, 3, 3]
factors = [2, 4, 2, 4, 2, 4]
scale_vs_channels = [1, 2, 1, 2, 1, 2]
bottleneck_blocks = 3
outputs_unshift_down/output9

# params: 13.40M
<9 GB
results still not great

overall pitch shifting seems like quite the difficult task for this model to learn