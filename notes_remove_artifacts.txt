just realized val dl not in test mode

new runs for that
bigger / deeper model better after moving to test mode
larger batch size also better

now let's try 2 tweaks:

1. swap to l1 loss on waveform  (as the MDX model does for vocal isolation, and it does very well)
2. remove the impossible frequencies.  e.g. if shifted up then down, top freqs are missing.  other direction we just lose fidelity, not all freqs.
removing them for shifting up is important because then the model tries to predict missing freqs, as that is a lot of the loss and easy to do, but doesn't actually improve quality

(output17)
First, doing 2., as it is likely more important for quality.


(output18)
trying 1., result after 10k steps is much worse sounding audio, lots of background noise

let's try CDPAM loss, a perceptual loss

CDPAM loss takes too much memory, 1/2 the sequence length to 32256 from 65024 samples

still too much memory, 1/2 batch size from 32 to 16

(output19)
really bad results, but checked and cdpam expects sample rate of 22050
Since the lower frequencies are where the problem is, let's try resampling it 

(output20)
terrible
somehow sounds even worse
super loud as well

new idea:
rather than estimate the final output
estimate the difference
this way the model doesn't have to copy the input over AND do the other stuff
maybe it will be better?

worse, definitely worse.

output22:
back to original method, reduce sequence length to 3/4 original, so 48640 instead of 65024, gives a little speedup and still processes a decent bit of audio
up the # of lowpass filters to 10 from 6, change to using a for loop (should have done that from the beginning)
back to batch size 32

not really any better.

Let's move from spectrogram models to trying out waveform models
output23:
much faster
bottlenecked completely by data loading
lower memory usage
easier to deal with padding for inference due to no spec -> inv spec
similar setup to spectrogram model but made convs 1d

model config:
channels = [1, 8, 64, 256, 512]
blocks = [2, 2, 3, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]

not too much worse than spectrogram model on spectrogram val loss
(0.515 vs 0.491)
audio sounds pretty similar, no real improvement in sound

output24:
new change: ConvNext blocks have convs of size 7.  7x7 works for images, but 7x7 = 49, so conv of 41 is a similar prime # conv size for 1d
should allow model to see higher freqs and move information along the sequence faster
due to convnext structure this doesn't add many params (only +~200k on a ~21.5M param model)
similarly fast training (bottlenecked by data loading)

slightly worse results

output25:
since model is so fast and results are still worse than spec model,
let's add more blocks
blocks = [4, 4, 4, 4]
now all depths have same # of blocks
keeping the larger kernel for now, not clear why it would be worse esp for waveform audio model
e.g. hifigan does good with a kernel of 11 with 11 dilation giving huge effective kernel

training is slower but still somewhat dataloading bound
still low memory use, <5GB

we see increase in val loss after only 5k steps, and little to no progress after 1k steps
maybe the issue is too high of lr?

reduce lr: muon from 2e-3 to 1e-3, adamw from 3e-4 to 1e-4

results noticably worse

increase lr?
output27
muon to 5e-3, adamw to 5e-4
much better results
at 5k steps achieves val loss of 0.508, which is better than any other wav model attempt after 10k steps (next best @ 10k steps, 0.5153, output23)

new best result, beating even spectrogram model in this setting.

output28
let's try raising lr 2x:
muon to 1e-2, adamw to 1e-3

worse.

more depth:
output29
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 32, 128, 256, 512]
blocks = [4, 4, 4, 4, 4]
factors = [8, 4, 4, 2, 2]
scale_vs_channels = [1, 1, 1, 1, 1]

model a little bigger, a little more memory, <6GB

less depth:

output30:
from:
channels = [1, 8, 64, 256, 512]
blocks = [4, 4, 4, 4]
factors = [8, 8, 4, 2]
scale_vs_channels = [1, 1, 1, 1]
to:
channels = [1, 8, 64, 128, 256]
blocks = [4, 4, 4, 4]
factors = [8, 8, 2, 2]
scale_vs_channels = [1, 1, 1, 1]

much smaller (from ~22M params to ~6M params)
faster
worse

looking at bigvgan paper, a few things to try
1. gradient clipping:

grad clip norm of 1e3, same as bigvgan paper uses, 
output31

maybe track grad norms to see whats typical?

very similar results
makes sense, model wasn't unstable. so it didn't really have much of an effect
model val loss was monotonically decreasing though, so that's nice

looking at DAC (neural audio codec) paper, they do various losses, a spectrogram loss, feature matching, etc
the GAN approach is tempting, it has been proven to give great results in audio
but, the real idea is from melgan, multi level feature matching loss on discriminator.
but training discriminator is expensive and takes 1-2 MILLION steps to converge

instead, let's use some of the encoding layers from a neural codec like DAC or EnCodec.
Pros: Due to compression / reconstruction objective, tries to model the most perceptually relevant parts of the signal already
is a pretrained model
can handle high sample rate (trained on 44.5khz, 48khz )

output32
new: dac_loss.py implementing DACFeatureMatchingLoss
unfortunately using all layers + the 49152 samples destroyed memory
reducing to after just the first EncoderBlock (unfortunately first encoder block is without any downsampling so memory usage quite bad) and 16384 samples makes it fit in <12GB memory
given model was using <5-6GB before and went to using >24GB, something like 75% of memory use here is from the DAC loss :/

promising, had a different sound to it than spec loss
but plataeued and val spec loss increased then plataeued, didn't decrease
output33
trying multiple encoder blocks, may be able to fit in memory
can only fit 2 blocks
output34
new idea, feature match loss after every residual block
just use first block

quality wise they sound softer, less of the harsh robotic tone that the pitch shifting causes, but with weird artifacts
let's try this with the multi scale stft loss, so both losses at once
using just first block with feat matching loss after every residual
the scale of the stft loss is much larger (100x) than the scale of the feature matching loss, 
initially trying with no scaling of either loss to see if the strong stft loss can prevent the artifacts from the feature matching loss
output35

prevents the artifacts, but doesn't really sound better

let's try the multiscale mel spec loss from the DAC paper
they note that you need different # mel bins to prevent "holes" in the spectrogram
and that the shorter window lengths are important for transients
so this may help

output36

maybe? idk hard to tell with short audios

let's try same length as before the DACFeatureMatchingLoss, 16384*3 samples

output37

muon is supposed to have worse later convergence, let's try out this PSGD optimizer

gonna have to try a few things to tune hparams
noticably slower, probably due to not compiling (windows moment)

lots of loss spikes after like 1k steps with lr of 1e-3
output38

trying 3e-4 lr
bit of a slow start, and stalls out later
output39
trying 5e-4 lr
worse overall, loss spikes later
output40
maybe lower lr?
1e-4
ends up worse than other lrs, but monotonically decreasing val loss at least
output41

back to 3e-4, try out different momentum
default was 0.9, let's try 0.8
initially promising results, but falls behind in the 2nd half and val loss goes up and down later on
output42

default of 0.9, let's try 0.95
ends up very similar to the others tbh
output43

new dataset setup, preprocess data instead of on the fly shifting
to reduce disk usage, reduce # of shifts
# only deal with large shifts for now
shift = [-12, -11, -10, 10, 11, 12]
the largest shifts are the ones we care about most anyways
since they are the ones that have the most artifacts
preprocessing does 2x the number of lowpass filters since might as well (more filters = faster / better rolloff / smaller transition from allowed to blocked) (and they are not the slow part anyways)

have to redo comparisons

Muon:
Muon(muon_params, lr=5e-3, momentum=0.95,
                 adamw_params=adamw_params, adamw_lr=5e-4, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

also due to faster dataloading will increase # of train steps to 20k
(went from ~2.6it/s just loading data with 2 workers to almost 40it/s just loading data)
finally reduced model first level to only 3 blocks intead of 4 to try to speed it up a bit

output44

PSGD:

ForeachPSGDKron(model.parameters(), lr=3e-4, beta = 0.95, weight_decay=0.01)

output45

Let's try out the SOAP optimizer.  as a 2nd order optimizer its supposed to be quite good
use adam settings:
lr 5e-4, betas = 0.9, 0.95
output46

Let's try out plain old AdamW, to compare against all these other optimizers
lr 5e-4, betas = 0.9, 0.95
output47

AdamW and SOAP underperformed vs Muon & PSGD significantly

PSGD underperformed a little

AdamW lr may be incorrect
lets try increasing it (SOAP is slower so use AdamW to tune first)

AdamW lr 1e-3
much better, results got much closer to psgd / muon
output48

AdamW lr 2e-3
similar results but faster beginning
output49

AdamW lr 5e-3
immediate and constant loss explosions
deleted run since no useful information

will use 1e-3 as adam lr

let's try to tune muon a bit more now with new adam lr and set muon to 10x adam lr
so muon from 5e-3 to 1e-2

optimizer = Muon(muon_params, lr=1e-2, momentum=0.95,
                adamw_params=adamw_params, adamw_lr=1e-3, adamw_betas=(0.90, 0.95), adamw_wd=0.01)

much better, beats all previous runs by 6k steps / 20k
output50

let's try SOAP with new lr 1e-3

does OK
massive loss spike ~12k-13k steps in
stalls out for a while
gets a nice improvement in val loss in the last 2k steps, but too late
although does manage to beat SOAP with lower lr at the very end
output51

let's try PSGD with higher lr, all optimizers are doing better with higher lr with new dataset setup

lr = 1e-3

loss explosion pretty quickly

lr = 5e-4?
basically same result as 3e-4 lr
output52

OK, let's try much longer training runs.
AdamW, 100k steps
output53

Muon
output54

PSGD
output55

just realized that the audio isn't getting properly low-passed for pitch shifted baseline


fix audio preprocessing
output56

fix preprocessing actually (Muon)
output57

try longer run to see if better result (Muon)
not really, a little better
output58

add seeds / reproducibility, reduce LR a bit
output59

try out wavlm feature matching loss
the paper "FINALLY: fast and universal speech enhancement with studio-like quality" uses wavlm conv features
they show that wavlm features allow them to train a decent quality autoencoder, notably CDPAM completely fails to train an autoencoder (which matches my experience)
they come up with metrics to measure how good features are and show that wavlm performs well on those metrics (CDPAM fails them)

they say that wavlm features + stft loss allows them to train a model with an MOS score approaching that of adversarial training (4.3 vs 4.6) without any discriminator
problem: wavlm features only work for 16khz.  so they use wavlm features for stage 1 of 3, and then not for later stages which use discriminators and 48khz audio

for this test I will just resample audio to 16khz in dataloader and reduce # of samples processed to 1/3 (same factor as sample rate reduction)
if it works, I will (in another repo) attempt to distill wavlm conv features into another model

48khz audio -> student -> features
16khz audio -> wavlm_conv -> features
loss on features
in theory gives us student that can handle 48khz audio and has good properties like wavlm
but first need to see if worth it to even try

does manage to train model
results not noticably better than multi stft loss
metrics worse, not surprising, very different loss function for training vs val
output60

let's try wavlm features + the melspec loss i was using before
the wavlm loss gets down to ~7e-6 by the end of 10k steps
the melspec loss gets down to ~0.97 = ~9.7e-1 by the end of 10k steps
to keep the loss scales similar, multiply wavlm loss by 1e5
output61

train for longer
didn't do better
output62

try fixing skip connections
1. conv that takes channels * 2 and outputs channels (kernel = 1)
also added sisdr for validation, it is inverted since it is a loss, so lower is better
since nothing is directly optimizing anything like sisdr, it should be more useful to tell quality
output63

2. conv on the residual, then add
like conv(res) + x
output64

either way performance not great.  maybe this architecture is bad
let's try melgan based architecture
(good audio archs are often very different from good vision archs)
rough start but caught up by the end
output65

some tweaks to fix the architecture, it wasn't quite correct
worse somehow
also both melgan based models had high pitched artifacts
output66

many sound separation models predict a mask they then multiply
try that at end of model? start with melgan
model NaN'd out ~300 steps in, gradient norms died to 0 after ~65 steps
output67

let's try again with original unet model?
trained successfully, but worse results
output68

all these tweaks and the model is still failing to fix the audio
to make the task easier, let's make training data only have 1 shift, same as test (the 1 octave shift)
remove wavlm loss, try with just stft, to make training / iterating faster

output69

overall, the artifacts are not going away.
arch, loss, optimizer, 10k steps or 100k steps, still same artifact as original audio

let's try GAN training, there is a reason every audio paper uses one

notes_gan.txt


ok, let's try model_1d.py without multiplication
muon, 5k steps, batch 32
output70

modified architecture following more closely (but not the same) setup as hifi++ wavunet
same otherwise
wow, really good spectrogram matching, the melspec looks identical
but still sounds bad
output71

l1 loss? modified arch
not really, l1 loss just isn't very great perceptually
output72

if not the power spectra then its the phase!
phase loss from dac library
killed after 2k steps... my ears!!!!
output73

modified arch,  melspec + l1 loss
lambda_l1 = 10, since its ~0.025 and melspec is ~1 after training, brings them within 1 oom
looks like the l1 might be helping
output74

20k steps
better, sounds slightly better than baseline
but nowhere near original
output75

modified the v2 model to be faster and have more params (less blocks at high res, more channels at low res, more downsampling)
5k steps
just mel spec loss
similar metrics
output 76

back to the melspec + l1 setting but with 100k steps and bigger batch size (64)
since its the only result that had real promise for making the audio sound actually good
and it was still having loss / metric improvements at 20k steps

really great results actually
unfortunately after testing on a full audio there was artifacts
maybe due to too short training length

output77

so:
increase train length 4x to 65536 (like bigvgan v2 config)
reduce batch size 4x
increase the validation length to 16*16384 (~5.4 seconds, close to the length that caused issues)
output78

realization: if you upsample the audio to 96khz before shifting, then you can keep all the high frequencies intact for any shift <= +1 octave (+12 semitones)
so redo data preprocessing to fix this
this way model can be trained to always output fullband audio.
during testing, when testing on a sample, the scores for the model were actually worse than the shifted audio
as due to the low pass filter on training data, the model actually removed some of the high frequencies still present
10k steps: does it work
output79: yes

100k steps
output80

let's try increasing batch size a bit, from 16 to 32, since muon benefits from larger batch size
maybe reduce data size from 4*16384 to 3*16384 to help speed up training a bit (still ~1 second of audio so it sees all frequencies)
also reduce val length from 16x to 12x 16k, the >5 seconds length ends up creating a lot of padding for shorter audios, and makes eval kinda slow
pretty good results
output81

installed triton for windows, see how much faster training goes
train for 5k steps, muon should be compiled.  based on previous test the model inferences a batch at ~19it/s
so ideally training speed is 1/3 that, but loss/optim takes some time
training at ~4.2-4.3 it/s
now trying compiling the model
when testing on inference in the model testing code, speeds up model by ~2x to ~35it/s!
only speeds up training to ~5it/s
either optimizer has large overhead
or mel loss has large overhead
without mel loss gets to ~5.5it/s
without mel loss and without muon (AdamW, but not fused adam) gtes to ~8.1-8.2it/s
adamw with mel loss gets ~7.1-7.2it/s (fused has similar speed)
so muon has large overhead here
let's try adamw for 100k steps to see if with enough steps it approaches muon results
output82

since the unshift_down work has found a better architecture setup, inspired by the DAC setup, let's see if it works better for removing artifacts as well.
and switch back to muon for it
# params: 34.92M
channels = [48, 96, 192, 384, 768]
blocks = [3, 3, 3, 3]
factors = [2, 2, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 4
output83
short run to test it out, 10k steps instead of usual 100k
a little worse than output81

maybe try alternative option:
less patching, but fewer channels to maintain speed
channels = [16, 32, 128, 256, 512]
blocks = [3, 3, 3, 3]
factors = [2, 4, 4, 8]
scale_vs_channels = [1, 1, 2, 4]

bottleneck_blocks = 3

patching = 2
output84
trains faster than other option
decent results

muon has had an update, adds weight decay, try it out
had to fix by commenting out distributed stuff
also terrible results
reverted to commit 92bc50fb8749f12cd3869ce60c984c3bbe844cf5
and it works again
trying commit 28c793b55ef1cf86e5d6091bfbdbe0029b11eabb (next commit)
works fine
trying commit eb225792d1da44848dff39f9fac9758a71ee3f18 (next commit)
doesn't work (due to distributed stuff)
trying commit fc20399079ef3570ecf7ba2f12cbb82cc3464e86 (next muon.py commit)
breaks, but had to comment out some code about all gather
maybe need to actually do distributed stuff
can't on windows
modified the code to work without all gather, see how it does
output85
not as good :/
big spike in grad norm, loss is worse and has a small bump

try 2 of modification 
output86
still not working
initially gets ok si-sdr but then just fails to learn


swap to local editable install
latest commit + my modification for non distributed
output87

learns, but is noticably worse
loss is pretty similar but a bit worse
val metrics are worse and they aren't improving
handful of large spikes in grad norm

checkout commit 92bc50fb8749f12cd3869ce60c984c3bbe844cf5
try that
output 88 
works

ok, got advice from eleutherai discord
wrapped convs in class that reshapes the weights
since new muon handles 3d weights differently now
now trying to train with that setup
output89
works! and results are slightly better!

For average improvement benchmark
with output84:
 E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - -4.64016886295264 Subtraction - -13.569113724843126
Step:  2000
Average improvements: Model - -4.604981892745814 Subtraction - -13.569113724843126
Step:  3000
Average improvements: Model - -9.264409568461618 Subtraction - -13.569113724843126
Step:  4000
Average improvements: Model - -4.92021777790467 Subtraction - -13.569113724843126
Step:  5000
Average improvements: Model - -5.6688230394693395 Subtraction - -13.569113724843126
Step:  6000
Step:  7000
Average improvements: Model - -8.447322318212168 Subtraction - -13.569113724843126
Step:  8000
Average improvements: Model - -11.264506773855418 Subtraction - -13.569113724843126
Step:  9000
Average improvements: Model - -9.866973060463645 Subtraction - -13.569113724843126
Step:  10000
Average improvements: Model - -10.613895883690292 Subtraction - -13.569113724843126


With output89:
E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - 0.31719582521259504 Subtraction - -13.569113724843126
Step:  2000
Average improvements: Model - -1.5929669580740597 Subtraction - -13.569113724843126
Step:  3000
Average improvements: Model - -4.639959124448916 Subtraction - -13.569113724843126
Step:  4000
Average improvements: Model - -5.794628661815162 Subtraction - -13.569113724843126
Step:  5000
Average improvements: Model - -10.138142818013568 Subtraction - -13.569113724843126
Step:  6000
Average improvements: Model - -7.555109908064983 Subtraction - -13.569113724843126
Step:  7000
Average improvements: Model - -6.107469998312544 Subtraction - -13.569113724843126
Step:  8000
Average improvements: Model - -8.13147475796058 Subtraction - -13.569113724843126
Step:  9000
Average improvements: Model - -7.005445790777264 Subtraction - -13.569113724843126
Step:  10000
Average improvements: Model - -6.731054338322272 Subtraction - -13.569113724843126


Better at most steps, at step 1000 has a slightly positive score!


Now, output90
same as output89, with updated muon
and using the new feature, weight decay in muon!
I had set it to 0 for testing, but default is 0.01, and the LLM muon 2x speed paper found it critical
so setting it to 0.01, same as adamw

similar results, which is good / expected
should only really come into play in longer runs to reduce overfitting

PS E:\pitch_shifter> python .\scripts\evaluate_remove_artifacts.py
(312944,) 48000
Step:  1000
Average improvements: Model - -3.3375640283382975 Subtraction - -13.56
Step:  2000
Average improvements: Model - -9.085865686564793 Subtraction - -13.569
Step:  3000
Average improvements: Model - -6.631856726540372 Subtraction - -13.569
Step:  4000
Average improvements: Model - -9.119680613752772 Subtraction - -13.569
Step:  5000
Average improvements: Model - -8.239379045059371 Subtraction - -13.569
Step:  6000
Step:  8000
Average improvements: Model - -2.6199704498009506 Subtraction - -13.563724843126
Step:  9000
Average improvements: Model - -7.6327673905560065 Subtraction - -13.563724843126
Step:  10000
Average improvements: Model - -6.836417576744688 Subtraction - -13.569724843126

weight decay maybe hurts results a bit early on, should help with longer training

let's try 100k steps
output91




can we make a faster good model? faster training -> can do longer training runs with larger batch sizes
(batch 32, 3*16384 samples, 200 forward passes timed (randomized input each forward))
current model_1d_v2 setup:  Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 11.4979989528656 seconds

model_1d_dac Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 16.86350131034851 seconds

model_1d_spec Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 3.2860000133514404 seconds
this model is super fast but doesn't touch phase

model_1d_spec_phase Input shape torch.Size([32, 1, 49152]) Output shape torch.Size([32, 1, 49152]) Took 6.290999412536621 seconds
Does same thing to phase as spec with different set of params
unsurprisingly, takes twice as long, but since original is super fast, still pretty quick


head to head model comparisons
at end of training, do si-sdr on whole dataset, and production quality metric from audiobox aesthetics.
training for 20k steps at batch 32 with size of 3*16384 during training (~1 second 48khz), and 12*16384 during testing (~4.1 seconds 48khz)

also, the validation mel loss now matches training loss, already have si-sdr for alternative metric anyways
additionally added spectrogram visualization like in other setup, helps see if any obvious artifacts from model, and see difference in model artifacts between architectures

output92 is the model_1d_v2 model
output93 is the model_1d_dac model
first training of model_1d_dac exploded after ~2.2k steps
lowering lr from 5e-3 muon 5e-4 adamw to 1e-3 muon 1e-4 adamw (original DAC model uses 1e-4 as LR)
output94 is the model_1d_dac model, with lower lr (hopefully no loss / grad norm explosion)
output95 is the model_1d_spec model
output96 is the model_1d_spec_phase model

we will compare models by looking at the PQ and si-sdr values

What, mean, std, median, min, max

model_1d_v2:

Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.100630060727917, 0.3200090286296923, 7.1264989376068115, 5.572937965393066, 8.147261619567871
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -11.26012295092323, 9.334433749448385, -9.199989795684814, -73.98777770996094, 5.0

model_1d_dac

Original PQ, 7.397888841802504, 0.27755410045302487, 7.426485061645508, 5.6817240715026855, 8.255695343017578
Shifted PQ, 6.341449345559327, 0.6139276536274133, 6.361863374710083, 4.283082962036133, 8.050267219543457
Unshifted PQ, 7.156263238569694, 0.3083742135394702, 7.18623685836792, 5.835231304168701, 8.145949363708496
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.289102757941732, 9.227971339065173, -8.289327144622803, -67.4070053100586, 5.25086784362793

skipping the spec models since they are pretty bad, barely if at all improve over shifted in PQ and si-sdr

used a t-test website between model_1d_v2 and model_1d_dac
just using summary stats, not original values get this:

P value and statistical significance:
The two-tailed P value is less than 0.0001
By conventional criteria, this difference is considered to be extremely statistically significant.
Confidence interval:
The mean of 1D_V2 minus 1D_DAC equals -0.055633177841777000
95% confidence interval of this difference: From -0.068967065916809540 to -0.042299289766744460
Intermediate values used in calculations:
t = 8.1974
df = 8574
standard error of difference = 0.007 

so model_1d_dac is better in PQ
could have used paired t-test since we are working on same audio samples between both models, and all the values are saved

model_1d_v2 also has an issue in spectrogram, where there is a line across middle pretty strongly, probably related to the patching or upsampling / downsampling layers aliasing

Anyways, so now the problem is that the DAC-based model is kinda slow

DAC uses a different upsampling / downsampling than other model does, maybe can use that to reduce length initially?

Original DAC speed / memory usage:
19177MiB
~2.6it/s

DAC with 1 2x downsampling / upsampling inserted at beginning (stride = 2, initial_channels = 16, no change in model strides)
11320MiB
~4.0 it/s

let's train this DAC model and see how good it is, the speedup / memory reduction is quite nice, hopefully doesn't hurt performance too much
output97

also to note, DAC codebase configs use audio segments of 0.38seconds ~= 16384 samples, this setup is currently training on 3x that
could potentially reduce and get some speedup / memory reduction, could maybe increase batch size as well?
not clear if batchsize*#samples is what matters, or if more batchsize less samples is better, or if less batchsize more samples is better
EVA-GAN seems to think #samples more important, we can try those changes out later

DAC with the downsampling and nothing else
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.130573544929277, 0.315028272943776, 7.159674406051636, 5.63200569152832, 8.066680908203125
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.780254906943586, 9.169527232640597, -8.700427055358887, -79.99737548828125, 7.1225175857543945


P value and statistical significance:
The two-tailed P value equals 0.0001
By conventional criteria, this difference is considered to be extremely statistically significant.
Confidence interval:
The mean of DAC original minus DAC downsample equals 0.025689693640417000
95% confidence interval of this difference: From 0.012463007487406676 to 0.038916379793427325
Intermediate values used in calculations:
t = 3.8160
df = 8574
standard error of difference = 0.007 

about half the distance between the DAC model and the 1d_v2 model
so with the downsampling the DAC model is a little faster to train than the 1d_v2 model and still gets better results

Now, let's compare it when I reduce training seqlen, keeping batch size the same and not
so 2 more runs:

DAC 2x downsample, seqlen 1/3 = 16384, batch = 32 - output98
uses 4654MiB memory
~7.5it/s
note: GPU utilization seems to be hovering ~50-60%, so this is underutilizing GPU, which is why its not even 2x faster training
on the other hand, this potentially could allow to increase model size without slowdown, if it turns out this is a good training setup?
Also, maybe need to do some profiling to see what's the bottleneck in allowing full GPU utilization?
DAC 2x downsample, seqlen 1/3 = 16384, batch = 96 (3x) - output99

overall results: original setup is much better quality than other 2

what if: reduce batch size by 1/2? batch size seems to affect quality less than # samples

output100
pretty similar to 3x batch size 1/3 seqlen

now, since using the 2x initial downsample version of DAC, can either 2x channels or 2x batch and still fit in memory

output101 batch size 64

What, mean, std, median, min, max
Original PQ, 7.397892306878496, 0.2775508426006901, 7.426472425460815, 5.68196964263916, 8.255901336669922
Shifted PQ, 6.341451782010385, 0.6139195740798666, 6.361877202987671, 4.282406330108643, 8.05012321472168
Unshifted PQ, 7.1610852617826035, 0.30346479560322903, 7.191209554672241, 5.904861927032471, 8.084527015686035
Shifted SI-SDR, -18.974841413039492, 10.08822065452006, -16.851783752441406, -79.63766479492188, 2.459331512451172
Unshifted SI-SDR, -10.537573355833658, 9.672006840327851, -8.410950660705566, -75.99976348876953, 6.5902276039123535

PQ almost exactly the same as DAc model without downsampling, but takes almost exactly as much time to train
si-sdr slightly worse

output102 batch size 32, but 2x width:
before:

initial_channels = 16
channels = [32, 64, 128, 256, 512]

after:

initial_channels = 32
channels = [64, 128, 256, 512, 1024]

doubling channels slower than doubling batch
2x batch was ~2.4 it/s
2x channels is ~2.0 it/s
so instead of taking ~2hrs 25 minutes from first step to last, took ~3 hrs 5 minutes (2x batch size vs no downsample had similar time to train)
used ~22GB memory

What, mean, std, median, min, max
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.181432992664735, 0.3153689910842815, 7.208005905151367, 5.696078300476074, 8.035009384155273
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.13491231569174, 9.320697221763956, -8.027478218078613, -66.01622772216797, 6.286093711853027

basially the same PQ, slightly better si-sdr

what if use the doubled channels model, but do a 4x downsample?
higher param count, but the extra downsampling should offset the params somewhat
output103
~13GB
~3.2 it/s

What, mean, std, median, min, max
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.10712933696028, 0.32146332933431987, 7.141205549240112, 5.501664161682129, 8.103938102722168
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -10.571595639094003, 9.338811168241824, -8.568894863128662, -72.30490112304688, 7.473552227020264

What if the dac model blocks were deeper? instead of 3 deep, 4 deep?
in order to do this, need to change the dilation setup, otherwise the dilations get ridiculous
e.g.  a dilation of 27 with kernel size 7 goes across 169 samples, if at the lowest level which is 1024x down it would be 173056 samples across, or 3.6 seconds at 48khz, more than needed for the task
instead:
original dac dilations:
[1, 3, 9]
alternative being tried:
[1, 2, 4, 8] # power of 2, like wavenet
allows increasing depth by 1 without massively changing receptive field
output104: 2x downsample, same width, deeper / alt dilations
~14GB
~3.3it/s

What, mean, std, median, min, max
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.117081089695888, 0.31094515995870514, 7.148790597915649, 5.675454139709473, 8.015702247619629
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -11.002143055637406, 9.378870968464312, -9.0050630569458, -77.61739349365234, 5.776949882507324

Vs the 4x downsample, Worse si-sdr, very similar PQ, spectrogram looks worse (although the 4x has artifact in the middle bin due to the downsample)
~1GB more memory (~7% more memory) for slightly faster (~3% faster), probably not worth since may not be able to fit 2x batch size with the memory increase

New idea: since # samples per item more important to quality than batch size, reduce batch size and increase # samples per batch?
batch = 16 (1/2), # samples = 16384 * 4 (33% more)
back to DAC with 1 downsample baseline
output105
~9.5GB memory, ~5.1it/s

What, mean, std, median, min, max
Original PQ, 7.397897063351389, 0.2775520125421763, 7.426762819290161, 5.682155132293701, 8.256006240844727
Shifted PQ, 6.3414564816587005, 0.6139258788253759, 6.361539125442505, 4.28273868560791, 8.050226211547852
Unshifted PQ, 7.109018741258934, 0.31372212457822723, 7.13405966758728, 5.487709999084473, 8.053832054138184
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -11.784670782595205, 9.808654266499042, -9.644914150238037, -76.40694427490234, 6.1292877197265625

pretty bad, probably reduced total # samples too much

batch = 16 (1/2), # samples = 16384 * 6 (2x)

this is same # of total samples, can see if more samples helps, or if too low batch size hurts (muon is supposed to be helped by bigger batch size)
output106
~12.8GB
~3.95it/s

What, mean, std, median, min, max
Original PQ, 7.397896721736709, 0.2775515563940755, 7.4266557693481445, 5.682501316070557, 8.255830764770508
Shifted PQ, 6.341460106088154, 0.6139255917877597, 6.36174201965332, 4.282208442687988, 8.050172805786133
Unshifted PQ, 7.120326527575059, 0.3207937387079875, 7.153940439224243, 5.695596694946289, 8.035765647888184
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -11.287993096484026, 9.4252814722245, -9.230056762695312, -76.8836669921875, 5.387906074523926
worse than baseline both in si-sdr and PQ

Best results was 2x width DAC with the initial 2x downsample, next best was 2x batch size DAC with initial 2x downsample
Let's try the 2x width DAC with 2x downsample for a full 100k steps to see how much improvement we can expect by training for longer
(output102 is same setup but 20k steps)
output107, 100k steps

What, mean, std, median, min, max
Original PQ, 7.397896721736709, 0.2775515563940755, 7.4266557693481445, 5.682501316070557, 8.255830764770508
Shifted PQ, 6.341460106088154, 0.6139255917877597, 6.36174201965332, 4.282208442687988, 8.050172805786133
Unshifted PQ, 7.269454287059271, 0.3142545631334111, 7.3019280433654785, 3.727414131164551, 8.227209091186523
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -8.97001226255566, 9.357444793539331, -6.956748008728027, -77.5777587890625, 9.223884582519531

Best result yet, by increasing # train steps to 100k, brought mean PQ from ~7.18 to ~7.27, only ~0.14 away from the original audio mean PQ now
listening to it, the 20k steps run (output102) I can hear the difference between original and fixed, the 100k steps run (output107) I cannot hear a difference
(at least for the clip logged in tensorboard)

Before trying out more training steps, let's try out lr decay, since training slowed down greatly by end of run
linear decay to 0

20k steps for comparison purposes
output108, 20k steps, linear decay to 0

What, mean, std, median, min, max
Original PQ, 7.397896721736709, 0.2775515563940755, 7.4266557693481445, 5.682501316070557, 8.255830764770508
Shifted PQ, 6.341460106088154, 0.6139255917877597, 6.36174201965332, 4.282208442687988, 8.050172805786133
Unshifted PQ, 7.195255577675442, 0.3137228682552299, 7.224622011184692, 5.4418792724609375, 8.086467742919922
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -9.82679199883615, 9.053880173735394, -7.994324207305908, -78.3104248046875, 7.509464263916016

better, statistically significant
unpaired test got p < 0.05
paired test with 1999 samples (limit is 2k) got p < 0.0001

so linear decay is better than not, by about 0.015 mean PQ
If this holds up for a 100k step run, since the remaining gap between original audio PQ mean is only ~0.14, then that is ~10% reduction in the gap

The improvement starts becoming noticable in metrics around the 16k step mark, or about 80% through training
this makes sense, because at that point the lr decay would have dropped lr to ~20% of original, so it is starting to impact the training
Since the 20% mark for 100k steps has 20k steps remaining (the entire length of this training run!), the lr decay should have more opportunity to impact the final result

output109, 100k steps, linear decay to 0

What, mean, std, median, min, max
Original PQ, 7.397888841802504, 0.27755410045302487, 7.426485061645508, 5.6817240715026855, 8.255695343017578
Shifted PQ, 6.341449345559327, 0.6139276536274133, 6.361863374710083, 4.283082962036133, 8.050267219543457
Unshifted PQ, 7.310329051382506, 0.26103831113450315, 7.330963373184204, 5.977506637573242, 8.18637466430664
Shifted SI-SDR, -18.974841641741737, 10.088221835618912, -16.85178565979004, -79.63749694824219, 2.459331512451172
Unshifted SI-SDR, -8.482395598423905, 9.1091620735658, -6.631881475448608, -79.22564697265625, 8.787049293518066
Gap is now ~0.088

However, model completely fails on singing data
To rectify this, adding in the VocalSet dataset, which has permissive licensing (Creative Commons Attribution 4.0 International)

output110
Initial version, 20k steps with similar setup as before
Half of each batch is vctk, other half is vocalset
VCTK evals and vocalset evals are separate both during / after training
During eval, testing on -12 and +12 shifted audios
During training, using -12, -8, -5, -3, -2, -1, 0, 1, 2, 3, 5, 8, 12
Gives a good range of possible shifts, but saves some preprocessing time by not doing every shift
Especially as the most important shifts are smaller, since most normal singing corrections are around 1-2 semitones, not an octave off
The testing is at octave to test extreme values
20k steps to see how well model works initially, creating new baseline

training loss should be lower than previous runs, due to addition of easier shifts, including shift of 0

due to the batch being 1/2 vctk and 1/2 vocalset, the variance will be higher
additionally, there used to be 32 samples of same shift
now there are 11 possible shifts
so total # samples per shift per dataset is 22x lower
to solve this, decrease lr

before:
muon_lr = 1e-3, adamw_lr = 1e-4
now:
muon_lr = 2e-4, adamw_lr = 2e-5

Finally, due to 2x the number of samples in validation VCTK (shift -12 and shift +12), and introduction of vocalset to validation
to make sure validation doesn't take too much time, following speakers moved from validation to testing
p315, p360, p239, p271, p280, p306

overall, results poor and not really improving even after 8k steps

instead, try increasing batch
output111:
to solve this, increase batch size
to do that, have to decrease model width
channels = [16, 32, 64, 128, 256]
initial_channels = 8
batch_size = 128
also increase lr back to original
muon_lr = 1e-3, adamw_lr = 1e-4

still not learning well

instead, try only training on shifts of -12 and +12:

output112

improving at vctk, but getting worse at vocalset, despite equal weighting
probably vocalset is harder?

let's try just vocalset train set
output113
doesn't really work... weird.

just shifted by 12, vctk?
output114
seems to work properly.

weirdly, grad norm increases near end of training, despite the lr going down???
this should be similar setup to previous, maybe due to model being less wide?

output115:
try out the vocos model

works ok, but not as good
seems to have these line artifacts, not clear why

after like 10-12k steps, the grad norm becomes much more variable and grows... weird

output116:
vocos but increase kernel size on depthwise convs from 7 to 17, since 8 blocks of kernel 7 = (6*7) +7 = 49 receptive field
8 blocks with kernel 17 = (16 * 7) + 17 = 129 receptive field
takes almost exactly the same amount of time forward pass, and adds maybe 2% params, so pretty cheap change

NaNed.  vocos seems quite unstable, keeps having NaN

going back to DAC since not so unstable with NaNs

output117, back to DAC
reduce batch size to 32, increase width to 1/2 instead of 1/4
lr: muon=1e-3 adamw=1e-4

output118, still dac
but now vocalset instead of vctk

fails to train, vocalset seems to be harder

output119
try n_fft=768 like the guy said for vocos
# params: 35.25M

output120
back to n_fft = 1024, but
make input conv have correct kernel size of 7
# params: 42.09M

output121:
half the batch size to speed up training runs, also to match DAC batch size

output122
change loss function to a l1 loss directly on predicted spectrogram, since the model directly predicts it.
potentially could reduce the line artifacts?

output123:
l1 loss on spec + melspec loss, the l1 loss reduced the streaky artifacting on the spectrogram but overall had worse results

nope, didn't really fix things

output124:
increase model width to 1536, decrease expansion to 1.  The input / output dimensions are big, so by increasing model width, maybe will get better results
reduce expansion to prevent # params exploding too much
# params: 50.52M

nope, still poor quality.

output125:
back to DAC, get baseline

output126:
DAC, increase initial downsampling to 4x:
initial_stride = 4

output127
same as before with 4x downsampling
but change initial upsampling / downsampling.

Previously:

in conv (1ch to 16ch)
downsample (16ch to 32ch)
unet
upsample (32ch to 16)
outconv(16ch to 1ch)

instead:

in_conv / downsample (1ch to 32ch)
unet
upsample + outconv (32ch to 1ch)

~7.5GB vram
similar quality but faster to train vs output126


output128:
1.5x width, see if can match quality / time of 2x downsample
channels = [48, 96, 192, 384, 768]

memory: ~10GB

Very similar metrics to output125 but slightly faster
(1hr 19min vs 1hr 25 min)
